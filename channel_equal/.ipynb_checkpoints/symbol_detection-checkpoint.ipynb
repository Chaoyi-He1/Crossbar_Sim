{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eaded07d-d196-4c0d-9a90-ddaaba117169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import scipy.io as sio\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "import torch.distributed as dist\n",
    "import errno\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a09addd0-98ac-478b-a2ab-6a97d67e1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{value:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {}'.format(header, total_time_str))\n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f33a748a-e0d2-423a-ba52-d0c37f88fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "def transform_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "# create dataset class\n",
    "class channel_dataset(data.Dataset):\n",
    "    def __init__(self, data_path, use_noise: bool = False, is_train: bool = True):\n",
    "        super(channel_dataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.data = sio.loadmat(data_path)\n",
    "        self.use_noise = use_noise\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.trans = transform_data()\n",
    "\n",
    "        self.signals = self.data['filter_outputs']\n",
    "        self.symbols = self.data['tx_symbols']\n",
    "        self.symbols = np.reshape(self.symbols, (-1))\n",
    "\n",
    "        if use_noise:\n",
    "            noise = np.random.randint(-2, 3, size=self.signals.shape).astype(np.uint8)\n",
    "            self.signals += noise\n",
    "\n",
    "        self.signals = np.reshape(self.signals, (-1, 8, 16, 1)).astype(np.uint8)\n",
    "\n",
    "        self.num_samples = self.signals.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.symbols[index]).to(torch.long), self.trans(self.signals[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5e754f7-922f-41d1-ad12-20ab0c026214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector_CNN(nn.Module):\n",
    "    def __init__(self, output_size: int = 4):\n",
    "        super(Detector_CNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # (B, 1, 8, 16) -> (B, 16, 2, 4)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(8, 16, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(16, 16, 3, 1, 1),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * 4 * 16, 128),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(128, 128),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, signal_in):\n",
    "        return self.mlp(self.cnn(signal_in))\n",
    "\n",
    "class Detector_MLP(nn.Module):\n",
    "    def __init__(self, output_size: int = 4):\n",
    "        super(Detector_MLP, self).__init__()\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, signal_in):\n",
    "        return self.mlp(signal_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6138b13-3c84-45d0-be85-2c391e907450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_regularize(model, device, alpha=0.01, interval=0.05):\n",
    "    central_params = torch.arange(0.0, 1+interval, interval).to(device)\n",
    "    reg_params = {}\n",
    "    for c in central_params:\n",
    "        if c == 0.0:\n",
    "            reg_params[c] = [p.abs() < interval / 2 for n, p in model.named_parameters()]\n",
    "        if c == 1.0:\n",
    "            reg_params[c] = [p.abs() > 1 - interval / 2 for n, p in model.named_parameters()]\n",
    "        else:\n",
    "            reg_params[c] = [(c - interval / 2 <= p.abs()) & (p.abs() < c + interval / 2) for n, p in model.named_parameters()]\n",
    "\n",
    "    reg_loss = 0.0\n",
    "    for c, params in reg_params.items():\n",
    "        reg_loss += alpha * torch.sum(torch.stack([torch.sum((p[params[i]].abs() - c).abs()) for i, (n, p) in enumerate(model.named_parameters())]))\n",
    "\n",
    "    return reg_loss\n",
    "\n",
    "# train one epoch\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler):\n",
    "    model.train()\n",
    "    metric_logger = MetricLogger(delimiter=\"    \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    for symbols, signals in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        symbols, signals = symbols.to(device), signals.to(device).to(torch.float)\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # equalized_output = model(tx_pilot, rx_pilot, rx_input)\n",
    "            output = model(signals)\n",
    "            loss = F.cross_entropy(output, symbols)\n",
    "\n",
    "            if epoch >= 10:\n",
    "                loss += quantize_regularize(model, device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        # calculate binary accuracy\n",
    "        output_bits = torch.argmax(output, dim=-1)\n",
    "        accuracy = (output_bits == symbols).float().mean()\n",
    "        \n",
    "        metric_logger.update(loss=loss, lr=optimizer.param_groups[0][\"lr\"])\n",
    "        metric_logger.update(accuracy=accuracy)\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "def evaluate(model, data_loader, device, epoch, print_freq):\n",
    "    model.eval()\n",
    "    metric_logger = MetricLogger(delimiter=\"    \")\n",
    "    header = 'Test: Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            for symbols, signals in metric_logger.log_every(data_loader, print_freq, header):\n",
    "                symbols, signals = symbols.to(device), signals.to(device).to(torch.float)\n",
    "\n",
    "                output = model(signals)\n",
    "                loss = F.cross_entropy(output, symbols)\n",
    "\n",
    "                # calculate binary accuracy\n",
    "                output_bits = torch.argmax(output, dim=-1)\n",
    "                accuracy = (output_bits == symbols).float().mean()\n",
    "                \n",
    "                metric_logger.update(loss=loss)\n",
    "                metric_logger.update(accuracy=accuracy)\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ba6ed9-1417-4972-8193-ba9a6f893ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(snr, mode, cnn, noise):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # set seed\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    batch_size = 512\n",
    "    num_epoch = 100\n",
    "    lr = 0.001\n",
    "    lrf = 0.1\n",
    "\n",
    "    train_data_path = \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/shuffle/filtered_data_train.mat\"\n",
    "    val_data_path = \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/shuffle/filtered_data_val.mat\"\n",
    "\n",
    "    train_dataset = channel_dataset(train_data_path, use_noise=noise, is_train=True)\n",
    "    val_dataset = channel_dataset(val_data_path, use_noise=noise, is_train=False)\n",
    "\n",
    "    train_sampler = data.RandomSampler(train_dataset)\n",
    "    val_sampler = data.SequentialSampler(val_dataset)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, drop_last=True)\n",
    "    val_loader = data.DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler, drop_last=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    out_size = 4 if mode != 'qam16' else 16\n",
    "    \n",
    "    model = Detector_CNN(output_size=out_size).to(device) if cnn else Detector_MLP(output_size=out_size).to(device)\n",
    "\n",
    "    params_to_optimize = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params_to_optimize, lr=lr)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    lf = lambda x: ((1 + math.cos(x * math.pi / num_epoch)) / 2) * (1 - lrf) + lrf\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
    "    scheduler.last_epoch = 0\n",
    "\n",
    "    best_loss, best_epoch = math.inf, 0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epoch):\n",
    "        train_dict = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=200, scaler=scaler)\n",
    "        val_dict = evaluate(model, val_loader, device, epoch, print_freq=200)\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch >= 20 and val_dict['loss'] < best_loss:\n",
    "            best_loss = val_dict['loss']\n",
    "            best_epoch = epoch\n",
    "            save_file = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(save_file, \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/best_detection_model.pth\")\n",
    "            print(\"Save model from: \", epoch)\n",
    "\n",
    "    print(\"Total time: {}s\".format(int(time.time() - start_time)))\n",
    "    print(\"Best loss: {}\".format(best_loss), \";  Best Epoch: {}\".format(best_epoch))\n",
    "\n",
    "    # min, max of the model parameters\n",
    "    min_params, max_params = [], []\n",
    "    for p in model.parameters():\n",
    "        min_params.append(p.min().item())\n",
    "        max_params.append(p.max().item())\n",
    "    min_params, max_params = np.min(min_params), np.max(max_params)\n",
    "    print(f\"Min of model parameters: {min_params}, Max of model parameters: {max_params}\")\n",
    "    # plot histogram of model parameters\n",
    "    params = [p.detach().cpu().numpy().flatten() for p in model.parameters()]\n",
    "    plt.hist(np.hstack(params), bins=100)\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bbbc864-274f-428a-8fa4-31ca0339d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = True\n",
    "noise = True\n",
    "\n",
    "snr = 15\n",
    "mode = 'psk4'  # pam4 qam4 qam16 psk4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8934e8df-2494-4147-a241-29d7ed4cde56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999498394863563 0.999849518459069\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "out_size = 4 if mode != 'qam16' else 16\n",
    "\n",
    "model = Detector_CNN(output_size=out_size).to(device) if cnn else Detector_MLP(output_size=out_size).to(device)\n",
    "model_path = \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/best_detection_model.pth\"\n",
    "ckpt = torch.load(model_path)['model']\n",
    "# for n, p in ckpt.items():\n",
    "#     print(n, p.shape)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()\n",
    "\n",
    "train_data_path = \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/shuffle/filtered_data_train.mat\"\n",
    "val_data_path = \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/shuffle/filtered_data_val.mat\"\n",
    "\n",
    "train_dataset = channel_dataset(train_data_path, use_noise=noise, is_train=True)\n",
    "val_dataset = channel_dataset(val_data_path, use_noise=noise, is_train=False)\n",
    "\n",
    "train_sampler = data.SequentialSampler(train_dataset)\n",
    "val_sampler = data.SequentialSampler(val_dataset)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=64, sampler=train_sampler, drop_last=False)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=64, sampler=val_sampler, drop_last=False)\n",
    "\n",
    "simulate_outputs = []\n",
    "true_symbols = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast('cuda'):\n",
    "        for symbols, signals in train_loader:\n",
    "            symbols, signals = symbols.to(device), signals.to(device).to(torch.float)\n",
    "\n",
    "            output = model(signals)\n",
    "\n",
    "            true_symbols.append(symbols.detach().cpu().numpy())\n",
    "            simulate_outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast('cuda'):\n",
    "        for symbols, signals in val_loader:\n",
    "            symbols, signals = symbols.to(device), signals.to(device).to(torch.float)\n",
    "\n",
    "            output = model(signals)\n",
    "\n",
    "            true_symbols.append(symbols.detach().cpu().numpy())\n",
    "            simulate_outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "simulate_outputs = np.concatenate(simulate_outputs, 0)  # (B, 4)\n",
    "true_symbols = np.concatenate(true_symbols, 0)  # (B,)\n",
    "\n",
    "HW_data_path = \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/hw_out_\" + str(snr) +\"db_\" + mode + \".npy\"\n",
    "HW_data = np.load(HW_data_path)  # (B, 4)\n",
    "\n",
    "simulate_acc = (np.argmax(simulate_outputs, -1) == true_symbols).mean()\n",
    "HW_acc = (np.argmax(HW_data, -1) == true_symbols).mean()\n",
    "\n",
    "print(simulate_acc, HW_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "0673b4f2-bd31-4b7a-899c-5457d1372331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]    [ 0/62]    eta: 0:00:02    lr: 0.001000    loss: 2.7788 (2.7788)    accuracy: 0.0547 (0.0547)    time: 0.0398    data: 0.0365    max mem: 21\n",
      "Epoch: [0] Total time: 0:00:02\n",
      "Averaged stats: lr: 0.001000    loss: 0.7729 (2.2844)    accuracy: 0.8906 (0.2486)\n",
      "Test: Epoch: [0]    [ 0/15]    eta: 0:00:00    loss: 0.7564 (0.7564)    accuracy: 0.8242 (0.8242)    time: 0.0358    data: 0.0351    max mem: 21\n",
      "Test: Epoch: [0] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.7334 (0.7516)    accuracy: 0.8711 (0.8430)\n",
      "Epoch: [1]    [ 0/62]    eta: 0:00:02    lr: 0.001000    loss: 0.7370 (0.7370)    accuracy: 0.8594 (0.8594)    time: 0.0393    data: 0.0361    max mem: 21\n",
      "Epoch: [1] Total time: 0:00:02\n",
      "Averaged stats: lr: 0.001000    loss: 0.1662 (0.2357)    accuracy: 0.9492 (0.9349)\n",
      "Test: Epoch: [1]    [ 0/15]    eta: 0:00:00    loss: 0.2741 (0.2741)    accuracy: 0.9102 (0.9102)    time: 0.0355    data: 0.0349    max mem: 21\n",
      "Test: Epoch: [1] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.2412 (0.2702)    accuracy: 0.9219 (0.9070)\n",
      "Epoch: [2]    [ 0/62]    eta: 0:00:02    lr: 0.000999    loss: 0.1592 (0.1592)    accuracy: 0.9375 (0.9375)    time: 0.0384    data: 0.0359    max mem: 21\n",
      "Epoch: [2] Total time: 0:00:02\n",
      "Averaged stats: lr: 0.000999    loss: 0.1670 (0.1612)    accuracy: 0.9453 (0.9424)\n",
      "Test: Epoch: [2]    [ 0/15]    eta: 0:00:00    loss: 0.2060 (0.2060)    accuracy: 0.9238 (0.9238)    time: 0.0359    data: 0.0351    max mem: 21\n",
      "Test: Epoch: [2] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1493 (0.1973)    accuracy: 0.9531 (0.9296)\n",
      "Epoch: [3]    [ 0/62]    eta: 0:00:02    lr: 0.000998    loss: 0.1860 (0.1860)    accuracy: 0.9219 (0.9219)    time: 0.0392    data: 0.0361    max mem: 21\n",
      "Epoch: [3] Total time: 0:00:02\n",
      "Averaged stats: lr: 0.000998    loss: 0.1430 (0.1516)    accuracy: 0.9453 (0.9444)\n",
      "Test: Epoch: [3]    [ 0/15]    eta: 0:00:00    loss: 0.2099 (0.2099)    accuracy: 0.9336 (0.9336)    time: 0.0357    data: 0.0351    max mem: 21\n",
      "Test: Epoch: [3] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1412 (0.2064)    accuracy: 0.9492 (0.9250)\n",
      "Epoch: [4]    [ 0/62]    eta: 0:00:02    lr: 0.000996    loss: 0.1513 (0.1513)    accuracy: 0.9434 (0.9434)    time: 0.0391    data: 0.0360    max mem: 21\n",
      "Epoch: [4] Total time: 0:00:02\n",
      "Averaged stats: lr: 0.000996    loss: 0.1136 (0.1457)    accuracy: 0.9590 (0.9466)\n",
      "Test: Epoch: [4]    [ 0/15]    eta: 0:00:00    loss: 0.1997 (0.1997)    accuracy: 0.9238 (0.9238)    time: 0.0358    data: 0.0351    max mem: 21\n",
      "Test: Epoch: [4] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1614 (0.1955)    accuracy: 0.9453 (0.9270)\n",
      "Epoch: [5]    [ 0/62]    eta: 0:00:02    lr: 0.000994    loss: 0.1509 (0.1509)    accuracy: 0.9395 (0.9395)    time: 0.0393    data: 0.0362    max mem: 21\n",
      "Epoch: [5] Total time: 0:00:02\n",
      "Averaged stats: lr: 0.000994    loss: 0.1503 (0.1439)    accuracy: 0.9570 (0.9471)\n",
      "Test: Epoch: [5]    [ 0/15]    eta: 0:00:00    loss: 0.2693 (0.2693)    accuracy: 0.9082 (0.9082)    time: 0.0356    data: 0.0349    max mem: 21\n",
      "Test: Epoch: [5] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1950 (0.2325)    accuracy: 0.9414 (0.9189)\n",
      "Epoch: [6]    [ 0/62]    eta: 0:00:02    lr: 0.000992    loss: 0.1661 (0.1661)    accuracy: 0.9395 (0.9395)    time: 0.0428    data: 0.0390    max mem: 21\n",
      "Epoch: [6] Total time: 0:00:02\n",
      "Averaged stats: lr: 0.000992    loss: 0.1444 (0.1430)    accuracy: 0.9375 (0.9464)\n",
      "Test: Epoch: [6]    [ 0/15]    eta: 0:00:00    loss: 0.2274 (0.2274)    accuracy: 0.9082 (0.9082)    time: 0.0352    data: 0.0346    max mem: 21\n",
      "Test: Epoch: [6] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1754 (0.2111)    accuracy: 0.9375 (0.9232)\n",
      "Epoch: [7]    [ 0/62]    eta: 0:00:02    lr: 0.000989    loss: 0.1302 (0.1302)    accuracy: 0.9434 (0.9434)    time: 0.0393    data: 0.0359    max mem: 21\n",
      "Epoch: [7] Total time: 0:00:02\n",
      "Averaged stats: lr: 0.000989    loss: 0.0996 (0.1443)    accuracy: 0.9609 (0.9456)\n",
      "Test: Epoch: [7]    [ 0/15]    eta: 0:00:00    loss: 0.2312 (0.2312)    accuracy: 0.9160 (0.9160)    time: 0.0352    data: 0.0345    max mem: 21\n",
      "Test: Epoch: [7] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1693 (0.2063)    accuracy: 0.9395 (0.9258)\n",
      "Epoch: [8]    [ 0/62]    eta: 0:00:02    lr: 0.000986    loss: 0.1826 (0.1826)    accuracy: 0.9258 (0.9258)    time: 0.0391    data: 0.0359    max mem: 21\n",
      "Epoch: [8] Total time: 0:00:02\n",
      "Averaged stats: lr: 0.000986    loss: 0.1347 (0.1420)    accuracy: 0.9512 (0.9462)\n",
      "Test: Epoch: [8]    [ 0/15]    eta: 0:00:00    loss: 0.2108 (0.2108)    accuracy: 0.9238 (0.9238)    time: 0.0352    data: 0.0346    max mem: 21\n",
      "Test: Epoch: [8] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1588 (0.1955)    accuracy: 0.9414 (0.9299)\n",
      "Epoch: [9]    [ 0/62]    eta: 0:00:02    lr: 0.000982    loss: 0.1499 (0.1499)    accuracy: 0.9395 (0.9395)    time: 0.0391    data: 0.0360    max mem: 21\n",
      "Epoch: [9] Total time: 0:00:02\n",
      "Averaged stats: lr: 0.000982    loss: 0.1139 (0.1371)    accuracy: 0.9688 (0.9481)\n",
      "Test: Epoch: [9]    [ 0/15]    eta: 0:00:00    loss: 0.2018 (0.2018)    accuracy: 0.9238 (0.9238)    time: 0.0357    data: 0.0350    max mem: 21\n",
      "Test: Epoch: [9] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1672 (0.1978)    accuracy: 0.9434 (0.9292)\n",
      "Epoch: [10]    [ 0/62]    eta: 0:00:06    lr: 0.000978    loss: 3.8924 (3.8924)    accuracy: 0.9473 (0.9473)    time: 0.1081    data: 0.0356    max mem: 21\n",
      "Epoch: [10] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000978    loss: 0.2505 (0.8636)    accuracy: 0.9531 (0.9494)\n",
      "Test: Epoch: [10]    [ 0/15]    eta: 0:00:00    loss: 0.1957 (0.1957)    accuracy: 0.9297 (0.9297)    time: 0.0356    data: 0.0349    max mem: 21\n",
      "Test: Epoch: [10] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1559 (0.1883)    accuracy: 0.9473 (0.9297)\n",
      "Epoch: [11]    [ 0/62]    eta: 0:00:06    lr: 0.000973    loss: 0.2641 (0.2641)    accuracy: 0.9512 (0.9512)    time: 0.1082    data: 0.0356    max mem: 21\n",
      "Epoch: [11] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000973    loss: 0.2610 (0.2413)    accuracy: 0.9297 (0.9502)\n",
      "Test: Epoch: [11]    [ 0/15]    eta: 0:00:00    loss: 0.2017 (0.2017)    accuracy: 0.9258 (0.9258)    time: 0.0357    data: 0.0350    max mem: 21\n",
      "Test: Epoch: [11] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1676 (0.1946)    accuracy: 0.9395 (0.9281)\n",
      "Epoch: [12]    [ 0/62]    eta: 0:00:06    lr: 0.000968    loss: 0.2151 (0.2151)    accuracy: 0.9492 (0.9492)    time: 0.1048    data: 0.0361    max mem: 21\n",
      "Epoch: [12] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000968    loss: 0.2317 (0.2157)    accuracy: 0.9434 (0.9499)\n",
      "Test: Epoch: [12]    [ 0/15]    eta: 0:00:00    loss: 0.1854 (0.1854)    accuracy: 0.9297 (0.9297)    time: 0.0354    data: 0.0347    max mem: 21\n",
      "Test: Epoch: [12] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1445 (0.1811)    accuracy: 0.9434 (0.9320)\n",
      "Epoch: [13]    [ 0/62]    eta: 0:00:06    lr: 0.000963    loss: 0.2161 (0.2161)    accuracy: 0.9492 (0.9492)    time: 0.1096    data: 0.0359    max mem: 21\n",
      "Epoch: [13] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000963    loss: 0.2029 (0.2054)    accuracy: 0.9434 (0.9506)\n",
      "Test: Epoch: [13]    [ 0/15]    eta: 0:00:00    loss: 0.1970 (0.1970)    accuracy: 0.9258 (0.9258)    time: 0.0356    data: 0.0349    max mem: 21\n",
      "Test: Epoch: [13] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1548 (0.1938)    accuracy: 0.9395 (0.9268)\n",
      "Epoch: [14]    [ 0/62]    eta: 0:00:06    lr: 0.000957    loss: 0.1863 (0.1863)    accuracy: 0.9609 (0.9609)    time: 0.1084    data: 0.0356    max mem: 21\n",
      "Epoch: [14] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000957    loss: 0.1845 (0.2004)    accuracy: 0.9609 (0.9503)\n",
      "Test: Epoch: [14]    [ 0/15]    eta: 0:00:00    loss: 0.1948 (0.1948)    accuracy: 0.9316 (0.9316)    time: 0.0356    data: 0.0349    max mem: 21\n",
      "Test: Epoch: [14] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1504 (0.1887)    accuracy: 0.9512 (0.9296)\n",
      "Epoch: [15]    [ 0/62]    eta: 0:00:06    lr: 0.000951    loss: 0.1920 (0.1920)    accuracy: 0.9570 (0.9570)    time: 0.1089    data: 0.0361    max mem: 21\n",
      "Epoch: [15] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000951    loss: 0.2058 (0.1942)    accuracy: 0.9512 (0.9516)\n",
      "Test: Epoch: [15]    [ 0/15]    eta: 0:00:00    loss: 0.1918 (0.1918)    accuracy: 0.9316 (0.9316)    time: 0.0358    data: 0.0351    max mem: 21\n",
      "Test: Epoch: [15] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1548 (0.1873)    accuracy: 0.9375 (0.9307)\n",
      "Epoch: [16]    [ 0/62]    eta: 0:00:06    lr: 0.000944    loss: 0.2123 (0.2123)    accuracy: 0.9473 (0.9473)    time: 0.1089    data: 0.0361    max mem: 21\n",
      "Epoch: [16] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000944    loss: 0.1811 (0.1918)    accuracy: 0.9551 (0.9497)\n",
      "Test: Epoch: [16]    [ 0/15]    eta: 0:00:00    loss: 0.2103 (0.2103)    accuracy: 0.9238 (0.9238)    time: 0.0356    data: 0.0349    max mem: 21\n",
      "Test: Epoch: [16] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1737 (0.2002)    accuracy: 0.9355 (0.9288)\n",
      "Epoch: [17]    [ 0/62]    eta: 0:00:06    lr: 0.000937    loss: 0.1840 (0.1840)    accuracy: 0.9551 (0.9551)    time: 0.1040    data: 0.0357    max mem: 21\n",
      "Epoch: [17] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000937    loss: 0.1813 (0.1882)    accuracy: 0.9492 (0.9498)\n",
      "Test: Epoch: [17]    [ 0/15]    eta: 0:00:00    loss: 0.1782 (0.1782)    accuracy: 0.9355 (0.9355)    time: 0.0357    data: 0.0350    max mem: 21\n",
      "Test: Epoch: [17] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1498 (0.1878)    accuracy: 0.9434 (0.9289)\n",
      "Epoch: [18]    [ 0/62]    eta: 0:00:06    lr: 0.000930    loss: 0.2025 (0.2025)    accuracy: 0.9492 (0.9492)    time: 0.1094    data: 0.0357    max mem: 21\n",
      "Epoch: [18] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000930    loss: 0.1687 (0.1871)    accuracy: 0.9492 (0.9508)\n",
      "Test: Epoch: [18]    [ 0/15]    eta: 0:00:00    loss: 0.1957 (0.1957)    accuracy: 0.9297 (0.9297)    time: 0.0382    data: 0.0375    max mem: 21\n",
      "Test: Epoch: [18] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1595 (0.1928)    accuracy: 0.9453 (0.9280)\n",
      "Epoch: [19]    [ 0/62]    eta: 0:00:06    lr: 0.000922    loss: 0.1763 (0.1763)    accuracy: 0.9492 (0.9492)    time: 0.1107    data: 0.0363    max mem: 21\n",
      "Epoch: [19] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000922    loss: 0.1785 (0.1827)    accuracy: 0.9473 (0.9510)\n",
      "Test: Epoch: [19]    [ 0/15]    eta: 0:00:00    loss: 0.2040 (0.2040)    accuracy: 0.9258 (0.9258)    time: 0.0354    data: 0.0347    max mem: 21\n",
      "Test: Epoch: [19] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1632 (0.1945)    accuracy: 0.9434 (0.9268)\n",
      "Epoch: [20]    [ 0/62]    eta: 0:00:06    lr: 0.000914    loss: 0.1826 (0.1826)    accuracy: 0.9453 (0.9453)    time: 0.1096    data: 0.0361    max mem: 21\n",
      "Epoch: [20] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000914    loss: 0.2044 (0.1807)    accuracy: 0.9453 (0.9508)\n",
      "Test: Epoch: [20]    [ 0/15]    eta: 0:00:00    loss: 0.1894 (0.1894)    accuracy: 0.9258 (0.9258)    time: 0.0370    data: 0.0361    max mem: 21\n",
      "Test: Epoch: [20] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1590 (0.1889)    accuracy: 0.9414 (0.9302)\n",
      "Save model from:  20\n",
      "Epoch: [21]    [ 0/62]    eta: 0:00:06    lr: 0.000906    loss: 0.1700 (0.1700)    accuracy: 0.9609 (0.9609)    time: 0.1099    data: 0.0392    max mem: 21\n",
      "Epoch: [21] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000906    loss: 0.1913 (0.1802)    accuracy: 0.9414 (0.9506)\n",
      "Test: Epoch: [21]    [ 0/15]    eta: 0:00:00    loss: 0.1833 (0.1833)    accuracy: 0.9316 (0.9316)    time: 0.0360    data: 0.0352    max mem: 21\n",
      "Test: Epoch: [21] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1512 (0.1868)    accuracy: 0.9434 (0.9297)\n",
      "Save model from:  21\n",
      "Epoch: [22]    [ 0/62]    eta: 0:00:06    lr: 0.000897    loss: 0.1919 (0.1919)    accuracy: 0.9375 (0.9375)    time: 0.1105    data: 0.0368    max mem: 21\n",
      "Epoch: [22] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000897    loss: 0.1669 (0.1842)    accuracy: 0.9492 (0.9489)\n",
      "Test: Epoch: [22]    [ 0/15]    eta: 0:00:00    loss: 0.2032 (0.2032)    accuracy: 0.9277 (0.9277)    time: 0.0364    data: 0.0356    max mem: 21\n",
      "Test: Epoch: [22] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1650 (0.1966)    accuracy: 0.9375 (0.9281)\n",
      "Epoch: [23]    [ 0/62]    eta: 0:00:06    lr: 0.000888    loss: 0.1641 (0.1641)    accuracy: 0.9473 (0.9473)    time: 0.1110    data: 0.0364    max mem: 21\n",
      "Epoch: [23] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000888    loss: 0.1850 (0.1807)    accuracy: 0.9512 (0.9506)\n",
      "Test: Epoch: [23]    [ 0/15]    eta: 0:00:00    loss: 0.2039 (0.2039)    accuracy: 0.9277 (0.9277)    time: 0.0362    data: 0.0355    max mem: 21\n",
      "Test: Epoch: [23] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1671 (0.2000)    accuracy: 0.9395 (0.9281)\n",
      "Epoch: [24]    [ 0/62]    eta: 0:00:06    lr: 0.000878    loss: 0.1507 (0.1507)    accuracy: 0.9570 (0.9570)    time: 0.1113    data: 0.0363    max mem: 21\n",
      "Epoch: [24] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000878    loss: 0.1584 (0.1770)    accuracy: 0.9668 (0.9504)\n",
      "Test: Epoch: [24]    [ 0/15]    eta: 0:00:00    loss: 0.2010 (0.2010)    accuracy: 0.9297 (0.9297)    time: 0.0363    data: 0.0356    max mem: 21\n",
      "Test: Epoch: [24] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1731 (0.1992)    accuracy: 0.9355 (0.9245)\n",
      "Epoch: [25]    [ 0/62]    eta: 0:00:06    lr: 0.000868    loss: 0.1912 (0.1912)    accuracy: 0.9434 (0.9434)    time: 0.1104    data: 0.0366    max mem: 21\n",
      "Epoch: [25] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000868    loss: 0.1762 (0.1781)    accuracy: 0.9512 (0.9498)\n",
      "Test: Epoch: [25]    [ 0/15]    eta: 0:00:00    loss: 0.1982 (0.1982)    accuracy: 0.9238 (0.9238)    time: 0.0356    data: 0.0349    max mem: 21\n",
      "Test: Epoch: [25] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1539 (0.1921)    accuracy: 0.9473 (0.9303)\n",
      "Epoch: [26]    [ 0/62]    eta: 0:00:06    lr: 0.000858    loss: 0.1795 (0.1795)    accuracy: 0.9473 (0.9473)    time: 0.1047    data: 0.0361    max mem: 21\n",
      "Epoch: [26] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000858    loss: 0.1787 (0.1755)    accuracy: 0.9414 (0.9505)\n",
      "Test: Epoch: [26]    [ 0/15]    eta: 0:00:00    loss: 0.2198 (0.2198)    accuracy: 0.9141 (0.9141)    time: 0.0352    data: 0.0344    max mem: 21\n",
      "Test: Epoch: [26] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1761 (0.2061)    accuracy: 0.9414 (0.9263)\n",
      "Epoch: [27]    [ 0/62]    eta: 0:00:06    lr: 0.000848    loss: 0.1619 (0.1619)    accuracy: 0.9551 (0.9551)    time: 0.1108    data: 0.0367    max mem: 21\n",
      "Epoch: [27] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000848    loss: 0.1588 (0.1764)    accuracy: 0.9570 (0.9511)\n",
      "Test: Epoch: [27]    [ 0/15]    eta: 0:00:00    loss: 0.2058 (0.2058)    accuracy: 0.9258 (0.9258)    time: 0.0351    data: 0.0344    max mem: 21\n",
      "Test: Epoch: [27] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1619 (0.1954)    accuracy: 0.9395 (0.9273)\n",
      "Epoch: [28]    [ 0/62]    eta: 0:00:06    lr: 0.000837    loss: 0.1405 (0.1405)    accuracy: 0.9668 (0.9668)    time: 0.1083    data: 0.0353    max mem: 21\n",
      "Epoch: [28] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000837    loss: 0.1778 (0.1739)    accuracy: 0.9531 (0.9507)\n",
      "Test: Epoch: [28]    [ 0/15]    eta: 0:00:00    loss: 0.1975 (0.1975)    accuracy: 0.9258 (0.9258)    time: 0.0365    data: 0.0358    max mem: 21\n",
      "Test: Epoch: [28] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1659 (0.1960)    accuracy: 0.9395 (0.9271)\n",
      "Epoch: [29]    [ 0/62]    eta: 0:00:06    lr: 0.000826    loss: 0.1822 (0.1822)    accuracy: 0.9336 (0.9336)    time: 0.1098    data: 0.0364    max mem: 21\n",
      "Epoch: [29] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000826    loss: 0.1725 (0.1721)    accuracy: 0.9492 (0.9499)\n",
      "Test: Epoch: [29]    [ 0/15]    eta: 0:00:00    loss: 0.1934 (0.1934)    accuracy: 0.9336 (0.9336)    time: 0.0361    data: 0.0353    max mem: 21\n",
      "Test: Epoch: [29] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1522 (0.1882)    accuracy: 0.9492 (0.9297)\n",
      "Epoch: [30]    [ 0/62]    eta: 0:00:07    lr: 0.000815    loss: 0.1386 (0.1386)    accuracy: 0.9531 (0.9531)    time: 0.1137    data: 0.0386    max mem: 21\n",
      "Epoch: [30] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000815    loss: 0.1376 (0.1753)    accuracy: 0.9629 (0.9491)\n",
      "Test: Epoch: [30]    [ 0/15]    eta: 0:00:00    loss: 0.1953 (0.1953)    accuracy: 0.9277 (0.9277)    time: 0.0351    data: 0.0344    max mem: 21\n",
      "Test: Epoch: [30] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1597 (0.1914)    accuracy: 0.9434 (0.9303)\n",
      "Epoch: [31]    [ 0/62]    eta: 0:00:06    lr: 0.000803    loss: 0.1895 (0.1895)    accuracy: 0.9414 (0.9414)    time: 0.1090    data: 0.0355    max mem: 21\n",
      "Epoch: [31] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000803    loss: 0.1542 (0.1742)    accuracy: 0.9551 (0.9504)\n",
      "Test: Epoch: [31]    [ 0/15]    eta: 0:00:00    loss: 0.1958 (0.1958)    accuracy: 0.9258 (0.9258)    time: 0.0348    data: 0.0341    max mem: 21\n",
      "Test: Epoch: [31] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1672 (0.1984)    accuracy: 0.9375 (0.9253)\n",
      "Epoch: [32]    [ 0/62]    eta: 0:00:06    lr: 0.000791    loss: 0.2078 (0.2078)    accuracy: 0.9395 (0.9395)    time: 0.1031    data: 0.0352    max mem: 21\n",
      "Epoch: [32] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000791    loss: 0.1589 (0.1698)    accuracy: 0.9609 (0.9506)\n",
      "Test: Epoch: [32]    [ 0/15]    eta: 0:00:00    loss: 0.1890 (0.1890)    accuracy: 0.9297 (0.9297)    time: 0.0395    data: 0.0387    max mem: 21\n",
      "Test: Epoch: [32] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1619 (0.1910)    accuracy: 0.9414 (0.9292)\n",
      "Epoch: [33]    [ 0/62]    eta: 0:00:06    lr: 0.000779    loss: 0.1802 (0.1802)    accuracy: 0.9551 (0.9551)    time: 0.1078    data: 0.0353    max mem: 21\n",
      "Epoch: [33] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000779    loss: 0.1354 (0.1707)    accuracy: 0.9590 (0.9507)\n",
      "Test: Epoch: [33]    [ 0/15]    eta: 0:00:00    loss: 0.2089 (0.2089)    accuracy: 0.9219 (0.9219)    time: 0.0351    data: 0.0344    max mem: 21\n",
      "Test: Epoch: [33] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1638 (0.1948)    accuracy: 0.9414 (0.9281)\n",
      "Epoch: [34]    [ 0/62]    eta: 0:00:06    lr: 0.000767    loss: 0.1877 (0.1877)    accuracy: 0.9453 (0.9453)    time: 0.1124    data: 0.0371    max mem: 21\n",
      "Epoch: [34] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000767    loss: 0.1423 (0.1694)    accuracy: 0.9668 (0.9504)\n",
      "Test: Epoch: [34]    [ 0/15]    eta: 0:00:00    loss: 0.2091 (0.2091)    accuracy: 0.9258 (0.9258)    time: 0.0385    data: 0.0377    max mem: 21\n",
      "Test: Epoch: [34] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1725 (0.1978)    accuracy: 0.9375 (0.9273)\n",
      "Epoch: [35]    [ 0/62]    eta: 0:00:06    lr: 0.000754    loss: 0.1646 (0.1646)    accuracy: 0.9551 (0.9551)    time: 0.1107    data: 0.0365    max mem: 21\n",
      "Epoch: [35] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000754    loss: 0.1618 (0.1684)    accuracy: 0.9512 (0.9512)\n",
      "Test: Epoch: [35]    [ 0/15]    eta: 0:00:00    loss: 0.1940 (0.1940)    accuracy: 0.9277 (0.9277)    time: 0.0357    data: 0.0350    max mem: 21\n",
      "Test: Epoch: [35] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1607 (0.1926)    accuracy: 0.9414 (0.9277)\n",
      "Epoch: [36]    [ 0/62]    eta: 0:00:06    lr: 0.000742    loss: 0.1565 (0.1565)    accuracy: 0.9492 (0.9492)    time: 0.1096    data: 0.0363    max mem: 21\n",
      "Epoch: [36] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000742    loss: 0.2317 (0.1656)    accuracy: 0.9316 (0.9513)\n",
      "Test: Epoch: [36]    [ 0/15]    eta: 0:00:00    loss: 0.1991 (0.1991)    accuracy: 0.9277 (0.9277)    time: 0.0352    data: 0.0345    max mem: 21\n",
      "Test: Epoch: [36] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1628 (0.1928)    accuracy: 0.9414 (0.9292)\n",
      "Epoch: [37]    [ 0/62]    eta: 0:00:06    lr: 0.000729    loss: 0.1482 (0.1482)    accuracy: 0.9551 (0.9551)    time: 0.1087    data: 0.0354    max mem: 21\n",
      "Epoch: [37] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000729    loss: 0.1634 (0.1674)    accuracy: 0.9473 (0.9505)\n",
      "Test: Epoch: [37]    [ 0/15]    eta: 0:00:00    loss: 0.2003 (0.2003)    accuracy: 0.9277 (0.9277)    time: 0.0350    data: 0.0343    max mem: 21\n",
      "Test: Epoch: [37] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1615 (0.1950)    accuracy: 0.9414 (0.9272)\n",
      "Epoch: [38]    [ 0/62]    eta: 0:00:06    lr: 0.000716    loss: 0.1900 (0.1900)    accuracy: 0.9395 (0.9395)    time: 0.1090    data: 0.0350    max mem: 21\n",
      "Epoch: [38] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000716    loss: 0.1632 (0.1663)    accuracy: 0.9648 (0.9504)\n",
      "Test: Epoch: [38]    [ 0/15]    eta: 0:00:00    loss: 0.2133 (0.2133)    accuracy: 0.9219 (0.9219)    time: 0.0356    data: 0.0349    max mem: 21\n",
      "Test: Epoch: [38] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1780 (0.2053)    accuracy: 0.9355 (0.9255)\n",
      "Epoch: [39]    [ 0/62]    eta: 0:00:06    lr: 0.000702    loss: 0.1645 (0.1645)    accuracy: 0.9531 (0.9531)    time: 0.1032    data: 0.0352    max mem: 21\n",
      "Epoch: [39] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000702    loss: 0.1890 (0.1642)    accuracy: 0.9355 (0.9511)\n",
      "Test: Epoch: [39]    [ 0/15]    eta: 0:00:00    loss: 0.1998 (0.1998)    accuracy: 0.9238 (0.9238)    time: 0.0361    data: 0.0354    max mem: 21\n",
      "Test: Epoch: [39] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1652 (0.1942)    accuracy: 0.9434 (0.9293)\n",
      "Epoch: [40]    [ 0/62]    eta: 0:00:06    lr: 0.000689    loss: 0.1669 (0.1669)    accuracy: 0.9492 (0.9492)    time: 0.1101    data: 0.0363    max mem: 21\n",
      "Epoch: [40] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000689    loss: 0.1624 (0.1661)    accuracy: 0.9570 (0.9505)\n",
      "Test: Epoch: [40]    [ 0/15]    eta: 0:00:00    loss: 0.2000 (0.2000)    accuracy: 0.9277 (0.9277)    time: 0.0350    data: 0.0343    max mem: 21\n",
      "Test: Epoch: [40] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1718 (0.2000)    accuracy: 0.9355 (0.9238)\n",
      "Epoch: [41]    [ 0/62]    eta: 0:00:06    lr: 0.000676    loss: 0.2197 (0.2197)    accuracy: 0.9355 (0.9355)    time: 0.1078    data: 0.0352    max mem: 21\n",
      "Epoch: [41] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000676    loss: 0.1798 (0.1656)    accuracy: 0.9492 (0.9508)\n",
      "Test: Epoch: [41]    [ 0/15]    eta: 0:00:00    loss: 0.2005 (0.2005)    accuracy: 0.9258 (0.9258)    time: 0.0379    data: 0.0372    max mem: 21\n",
      "Test: Epoch: [41] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1655 (0.1953)    accuracy: 0.9414 (0.9293)\n",
      "Epoch: [42]    [ 0/62]    eta: 0:00:06    lr: 0.000662    loss: 0.1679 (0.1679)    accuracy: 0.9531 (0.9531)    time: 0.1094    data: 0.0362    max mem: 21\n",
      "Epoch: [42] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000662    loss: 0.1645 (0.1641)    accuracy: 0.9531 (0.9505)\n",
      "Test: Epoch: [42]    [ 0/15]    eta: 0:00:00    loss: 0.1973 (0.1973)    accuracy: 0.9277 (0.9277)    time: 0.0351    data: 0.0344    max mem: 21\n",
      "Test: Epoch: [42] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1619 (0.1916)    accuracy: 0.9414 (0.9289)\n",
      "Epoch: [43]    [ 0/62]    eta: 0:00:06    lr: 0.000648    loss: 0.1552 (0.1552)    accuracy: 0.9492 (0.9492)    time: 0.1081    data: 0.0353    max mem: 21\n",
      "Epoch: [43] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000648    loss: 0.1292 (0.1621)    accuracy: 0.9609 (0.9508)\n",
      "Test: Epoch: [43]    [ 0/15]    eta: 0:00:00    loss: 0.1894 (0.1894)    accuracy: 0.9277 (0.9277)    time: 0.0350    data: 0.0343    max mem: 21\n",
      "Test: Epoch: [43] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1564 (0.1881)    accuracy: 0.9473 (0.9305)\n",
      "Epoch: [44]    [ 0/62]    eta: 0:00:06    lr: 0.000634    loss: 0.2036 (0.2036)    accuracy: 0.9414 (0.9414)    time: 0.1094    data: 0.0355    max mem: 21\n",
      "Epoch: [44] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000634    loss: 0.1385 (0.1614)    accuracy: 0.9609 (0.9512)\n",
      "Test: Epoch: [44]    [ 0/15]    eta: 0:00:00    loss: 0.1847 (0.1847)    accuracy: 0.9316 (0.9316)    time: 0.0355    data: 0.0348    max mem: 21\n",
      "Test: Epoch: [44] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1508 (0.1864)    accuracy: 0.9453 (0.9293)\n",
      "Save model from:  44\n",
      "Epoch: [45]    [ 0/62]    eta: 0:00:07    lr: 0.000620    loss: 0.1695 (0.1695)    accuracy: 0.9453 (0.9453)    time: 0.1135    data: 0.0361    max mem: 21\n",
      "Epoch: [45] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000620    loss: 0.1448 (0.1606)    accuracy: 0.9648 (0.9511)\n",
      "Test: Epoch: [45]    [ 0/15]    eta: 0:00:00    loss: 0.1832 (0.1832)    accuracy: 0.9297 (0.9297)    time: 0.0409    data: 0.0402    max mem: 21\n",
      "Test: Epoch: [45] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1484 (0.1848)    accuracy: 0.9453 (0.9299)\n",
      "Save model from:  45\n",
      "Epoch: [46]    [ 0/62]    eta: 0:00:06    lr: 0.000606    loss: 0.1723 (0.1723)    accuracy: 0.9453 (0.9453)    time: 0.1093    data: 0.0360    max mem: 21\n",
      "Epoch: [46] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000606    loss: 0.1580 (0.1588)    accuracy: 0.9453 (0.9509)\n",
      "Test: Epoch: [46]    [ 0/15]    eta: 0:00:00    loss: 0.2058 (0.2058)    accuracy: 0.9238 (0.9238)    time: 0.0361    data: 0.0354    max mem: 21\n",
      "Test: Epoch: [46] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1653 (0.1953)    accuracy: 0.9453 (0.9285)\n",
      "Epoch: [47]    [ 0/62]    eta: 0:00:06    lr: 0.000592    loss: 0.1756 (0.1756)    accuracy: 0.9473 (0.9473)    time: 0.1084    data: 0.0361    max mem: 21\n",
      "Epoch: [47] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000592    loss: 0.1428 (0.1592)    accuracy: 0.9609 (0.9510)\n",
      "Test: Epoch: [47]    [ 0/15]    eta: 0:00:00    loss: 0.2020 (0.2020)    accuracy: 0.9258 (0.9258)    time: 0.0351    data: 0.0344    max mem: 21\n",
      "Test: Epoch: [47] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1612 (0.1951)    accuracy: 0.9434 (0.9285)\n",
      "Epoch: [48]    [ 0/62]    eta: 0:00:06    lr: 0.000578    loss: 0.1837 (0.1837)    accuracy: 0.9492 (0.9492)    time: 0.1065    data: 0.0351    max mem: 21\n",
      "Epoch: [48] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000578    loss: 0.1536 (0.1607)    accuracy: 0.9531 (0.9505)\n",
      "Test: Epoch: [48]    [ 0/15]    eta: 0:00:00    loss: 0.1990 (0.1990)    accuracy: 0.9277 (0.9277)    time: 0.0352    data: 0.0344    max mem: 21\n",
      "Test: Epoch: [48] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1583 (0.1925)    accuracy: 0.9473 (0.9280)\n",
      "Epoch: [49]    [ 0/62]    eta: 0:00:06    lr: 0.000564    loss: 0.1646 (0.1646)    accuracy: 0.9434 (0.9434)    time: 0.1032    data: 0.0354    max mem: 21\n",
      "Epoch: [49] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000564    loss: 0.1526 (0.1577)    accuracy: 0.9570 (0.9513)\n",
      "Test: Epoch: [49]    [ 0/15]    eta: 0:00:00    loss: 0.2073 (0.2073)    accuracy: 0.9277 (0.9277)    time: 0.0350    data: 0.0343    max mem: 21\n",
      "Test: Epoch: [49] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1788 (0.2037)    accuracy: 0.9355 (0.9255)\n",
      "Epoch: [50]    [ 0/62]    eta: 0:00:06    lr: 0.000550    loss: 0.1746 (0.1746)    accuracy: 0.9473 (0.9473)    time: 0.1069    data: 0.0352    max mem: 21\n",
      "Epoch: [50] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000550    loss: 0.1630 (0.1577)    accuracy: 0.9414 (0.9517)\n",
      "Test: Epoch: [50]    [ 0/15]    eta: 0:00:00    loss: 0.1962 (0.1962)    accuracy: 0.9316 (0.9316)    time: 0.0360    data: 0.0353    max mem: 21\n",
      "Test: Epoch: [50] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1658 (0.1959)    accuracy: 0.9414 (0.9294)\n",
      "Epoch: [51]    [ 0/62]    eta: 0:00:07    lr: 0.000536    loss: 0.1708 (0.1708)    accuracy: 0.9512 (0.9512)    time: 0.1134    data: 0.0385    max mem: 21\n",
      "Epoch: [51] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000536    loss: 0.1870 (0.1563)    accuracy: 0.9355 (0.9506)\n",
      "Test: Epoch: [51]    [ 0/15]    eta: 0:00:00    loss: 0.1929 (0.1929)    accuracy: 0.9297 (0.9297)    time: 0.0379    data: 0.0372    max mem: 21\n",
      "Test: Epoch: [51] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1591 (0.1918)    accuracy: 0.9434 (0.9288)\n",
      "Epoch: [52]    [ 0/62]    eta: 0:00:06    lr: 0.000522    loss: 0.1560 (0.1560)    accuracy: 0.9434 (0.9434)    time: 0.1064    data: 0.0351    max mem: 21\n",
      "Epoch: [52] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000522    loss: 0.1475 (0.1574)    accuracy: 0.9492 (0.9508)\n",
      "Test: Epoch: [52]    [ 0/15]    eta: 0:00:00    loss: 0.1990 (0.1990)    accuracy: 0.9258 (0.9258)    time: 0.0349    data: 0.0342    max mem: 21\n",
      "Test: Epoch: [52] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1652 (0.1953)    accuracy: 0.9434 (0.9288)\n",
      "Epoch: [53]    [ 0/62]    eta: 0:00:06    lr: 0.000508    loss: 0.1528 (0.1528)    accuracy: 0.9609 (0.9609)    time: 0.1082    data: 0.0351    max mem: 21\n",
      "Epoch: [53] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000508    loss: 0.1400 (0.1572)    accuracy: 0.9492 (0.9507)\n",
      "Test: Epoch: [53]    [ 0/15]    eta: 0:00:00    loss: 0.2069 (0.2069)    accuracy: 0.9258 (0.9258)    time: 0.0358    data: 0.0351    max mem: 21\n",
      "Test: Epoch: [53] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1684 (0.1967)    accuracy: 0.9434 (0.9303)\n",
      "Epoch: [54]    [ 0/62]    eta: 0:00:06    lr: 0.000494    loss: 0.1680 (0.1680)    accuracy: 0.9375 (0.9375)    time: 0.1051    data: 0.0362    max mem: 21\n",
      "Epoch: [54] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000494    loss: 0.1278 (0.1553)    accuracy: 0.9648 (0.9503)\n",
      "Test: Epoch: [54]    [ 0/15]    eta: 0:00:00    loss: 0.2024 (0.2024)    accuracy: 0.9258 (0.9258)    time: 0.0357    data: 0.0350    max mem: 21\n",
      "Test: Epoch: [54] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1585 (0.1936)    accuracy: 0.9434 (0.9283)\n",
      "Epoch: [55]    [ 0/62]    eta: 0:00:06    lr: 0.000480    loss: 0.1760 (0.1760)    accuracy: 0.9375 (0.9375)    time: 0.1117    data: 0.0360    max mem: 21\n",
      "Epoch: [55] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000480    loss: 0.1755 (0.1532)    accuracy: 0.9453 (0.9515)\n",
      "Test: Epoch: [55]    [ 0/15]    eta: 0:00:00    loss: 0.1987 (0.1987)    accuracy: 0.9297 (0.9297)    time: 0.0353    data: 0.0346    max mem: 21\n",
      "Test: Epoch: [55] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1665 (0.1952)    accuracy: 0.9414 (0.9299)\n",
      "Epoch: [56]    [ 0/62]    eta: 0:00:07    lr: 0.000466    loss: 0.1467 (0.1467)    accuracy: 0.9609 (0.9609)    time: 0.1146    data: 0.0384    max mem: 21\n",
      "Epoch: [56] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000466    loss: 0.1576 (0.1532)    accuracy: 0.9492 (0.9513)\n",
      "Test: Epoch: [56]    [ 0/15]    eta: 0:00:00    loss: 0.1948 (0.1948)    accuracy: 0.9316 (0.9316)    time: 0.0359    data: 0.0351    max mem: 21\n",
      "Test: Epoch: [56] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1525 (0.1885)    accuracy: 0.9473 (0.9305)\n",
      "Epoch: [57]    [ 0/62]    eta: 0:00:06    lr: 0.000452    loss: 0.1413 (0.1413)    accuracy: 0.9609 (0.9609)    time: 0.1091    data: 0.0363    max mem: 21\n",
      "Epoch: [57] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000452    loss: 0.1285 (0.1525)    accuracy: 0.9570 (0.9507)\n",
      "Test: Epoch: [57]    [ 0/15]    eta: 0:00:00    loss: 0.2098 (0.2098)    accuracy: 0.9199 (0.9199)    time: 0.0359    data: 0.0351    max mem: 21\n",
      "Test: Epoch: [57] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1715 (0.1995)    accuracy: 0.9414 (0.9277)\n",
      "Epoch: [58]    [ 0/62]    eta: 0:00:07    lr: 0.000438    loss: 0.1582 (0.1582)    accuracy: 0.9434 (0.9434)    time: 0.1132    data: 0.0378    max mem: 21\n",
      "Epoch: [58] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000438    loss: 0.1566 (0.1530)    accuracy: 0.9473 (0.9509)\n",
      "Test: Epoch: [58]    [ 0/15]    eta: 0:00:00    loss: 0.1924 (0.1924)    accuracy: 0.9316 (0.9316)    time: 0.0361    data: 0.0354    max mem: 21\n",
      "Test: Epoch: [58] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1615 (0.1913)    accuracy: 0.9434 (0.9298)\n",
      "Epoch: [59]    [ 0/62]    eta: 0:00:06    lr: 0.000424    loss: 0.1338 (0.1338)    accuracy: 0.9629 (0.9629)    time: 0.1097    data: 0.0364    max mem: 21\n",
      "Epoch: [59] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000424    loss: 0.1348 (0.1513)    accuracy: 0.9629 (0.9513)\n",
      "Test: Epoch: [59]    [ 0/15]    eta: 0:00:00    loss: 0.1939 (0.1939)    accuracy: 0.9297 (0.9297)    time: 0.0354    data: 0.0347    max mem: 21\n",
      "Test: Epoch: [59] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1575 (0.1940)    accuracy: 0.9434 (0.9294)\n",
      "Epoch: [60]    [ 0/62]    eta: 0:00:06    lr: 0.000411    loss: 0.1655 (0.1655)    accuracy: 0.9453 (0.9453)    time: 0.1047    data: 0.0361    max mem: 21\n",
      "Epoch: [60] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000411    loss: 0.1188 (0.1517)    accuracy: 0.9668 (0.9510)\n",
      "Test: Epoch: [60]    [ 0/15]    eta: 0:00:00    loss: 0.2045 (0.2045)    accuracy: 0.9238 (0.9238)    time: 0.0352    data: 0.0345    max mem: 21\n",
      "Test: Epoch: [60] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1635 (0.1980)    accuracy: 0.9434 (0.9292)\n",
      "Epoch: [61]    [ 0/62]    eta: 0:00:06    lr: 0.000398    loss: 0.1701 (0.1701)    accuracy: 0.9492 (0.9492)    time: 0.1093    data: 0.0359    max mem: 21\n",
      "Epoch: [61] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000398    loss: 0.1609 (0.1504)    accuracy: 0.9453 (0.9513)\n",
      "Test: Epoch: [61]    [ 0/15]    eta: 0:00:00    loss: 0.1905 (0.1905)    accuracy: 0.9297 (0.9297)    time: 0.0363    data: 0.0356    max mem: 21\n",
      "Test: Epoch: [61] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1544 (0.1887)    accuracy: 0.9453 (0.9294)\n",
      "Epoch: [62]    [ 0/62]    eta: 0:00:06    lr: 0.000384    loss: 0.1858 (0.1858)    accuracy: 0.9316 (0.9316)    time: 0.1105    data: 0.0365    max mem: 21\n",
      "Epoch: [62] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000384    loss: 0.1507 (0.1490)    accuracy: 0.9473 (0.9510)\n",
      "Test: Epoch: [62]    [ 0/15]    eta: 0:00:00    loss: 0.2014 (0.2014)    accuracy: 0.9297 (0.9297)    time: 0.0359    data: 0.0352    max mem: 21\n",
      "Test: Epoch: [62] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1635 (0.1942)    accuracy: 0.9453 (0.9294)\n",
      "Epoch: [63]    [ 0/62]    eta: 0:00:06    lr: 0.000371    loss: 0.1523 (0.1523)    accuracy: 0.9551 (0.9551)    time: 0.1098    data: 0.0364    max mem: 21\n",
      "Epoch: [63] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000371    loss: 0.1484 (0.1491)    accuracy: 0.9531 (0.9515)\n",
      "Test: Epoch: [63]    [ 0/15]    eta: 0:00:00    loss: 0.1963 (0.1963)    accuracy: 0.9258 (0.9258)    time: 0.0362    data: 0.0355    max mem: 21\n",
      "Test: Epoch: [63] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1589 (0.1913)    accuracy: 0.9434 (0.9301)\n",
      "Epoch: [64]    [ 0/62]    eta: 0:00:06    lr: 0.000358    loss: 0.1154 (0.1154)    accuracy: 0.9609 (0.9609)    time: 0.1100    data: 0.0363    max mem: 21\n",
      "Epoch: [64] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000358    loss: 0.1467 (0.1483)    accuracy: 0.9570 (0.9513)\n",
      "Test: Epoch: [64]    [ 0/15]    eta: 0:00:00    loss: 0.1956 (0.1956)    accuracy: 0.9297 (0.9297)    time: 0.0360    data: 0.0353    max mem: 21\n",
      "Test: Epoch: [64] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1601 (0.1934)    accuracy: 0.9453 (0.9290)\n",
      "Epoch: [65]    [ 0/62]    eta: 0:00:06    lr: 0.000346    loss: 0.1453 (0.1453)    accuracy: 0.9512 (0.9512)    time: 0.1114    data: 0.0364    max mem: 21\n",
      "Epoch: [65] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000346    loss: 0.1467 (0.1468)    accuracy: 0.9512 (0.9515)\n",
      "Test: Epoch: [65]    [ 0/15]    eta: 0:00:00    loss: 0.2010 (0.2010)    accuracy: 0.9219 (0.9219)    time: 0.0374    data: 0.0365    max mem: 21\n",
      "Test: Epoch: [65] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1628 (0.1932)    accuracy: 0.9434 (0.9286)\n",
      "Epoch: [66]    [ 0/62]    eta: 0:00:06    lr: 0.000333    loss: 0.1699 (0.1699)    accuracy: 0.9355 (0.9355)    time: 0.1116    data: 0.0364    max mem: 21\n",
      "Epoch: [66] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000333    loss: 0.1250 (0.1469)    accuracy: 0.9648 (0.9516)\n",
      "Test: Epoch: [66]    [ 0/15]    eta: 0:00:00    loss: 0.1922 (0.1922)    accuracy: 0.9277 (0.9277)    time: 0.0357    data: 0.0349    max mem: 21\n",
      "Test: Epoch: [66] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1530 (0.1875)    accuracy: 0.9434 (0.9293)\n",
      "Epoch: [67]    [ 0/62]    eta: 0:00:06    lr: 0.000321    loss: 0.1539 (0.1539)    accuracy: 0.9570 (0.9570)    time: 0.1102    data: 0.0361    max mem: 21\n",
      "Epoch: [67] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000321    loss: 0.1366 (0.1466)    accuracy: 0.9590 (0.9517)\n",
      "Test: Epoch: [67]    [ 0/15]    eta: 0:00:00    loss: 0.1972 (0.1972)    accuracy: 0.9277 (0.9277)    time: 0.0376    data: 0.0367    max mem: 21\n",
      "Test: Epoch: [67] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1578 (0.1910)    accuracy: 0.9473 (0.9290)\n",
      "Epoch: [68]    [ 0/62]    eta: 0:00:07    lr: 0.000309    loss: 0.1246 (0.1246)    accuracy: 0.9648 (0.9648)    time: 0.1147    data: 0.0380    max mem: 21\n",
      "Epoch: [68] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000309    loss: 0.1369 (0.1459)    accuracy: 0.9570 (0.9515)\n",
      "Test: Epoch: [68]    [ 0/15]    eta: 0:00:00    loss: 0.2004 (0.2004)    accuracy: 0.9258 (0.9258)    time: 0.0362    data: 0.0354    max mem: 21\n",
      "Test: Epoch: [68] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1615 (0.1942)    accuracy: 0.9434 (0.9284)\n",
      "Epoch: [69]    [ 0/62]    eta: 0:00:06    lr: 0.000297    loss: 0.1315 (0.1315)    accuracy: 0.9531 (0.9531)    time: 0.1101    data: 0.0364    max mem: 21\n",
      "Epoch: [69] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000297    loss: 0.1633 (0.1461)    accuracy: 0.9375 (0.9513)\n",
      "Test: Epoch: [69]    [ 0/15]    eta: 0:00:00    loss: 0.1945 (0.1945)    accuracy: 0.9258 (0.9258)    time: 0.0392    data: 0.0385    max mem: 21\n",
      "Test: Epoch: [69] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1556 (0.1897)    accuracy: 0.9453 (0.9305)\n",
      "Epoch: [70]    [ 0/62]    eta: 0:00:06    lr: 0.000285    loss: 0.1400 (0.1400)    accuracy: 0.9531 (0.9531)    time: 0.1093    data: 0.0362    max mem: 21\n",
      "Epoch: [70] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000285    loss: 0.1538 (0.1464)    accuracy: 0.9492 (0.9511)\n",
      "Test: Epoch: [70]    [ 0/15]    eta: 0:00:00    loss: 0.1935 (0.1935)    accuracy: 0.9258 (0.9258)    time: 0.0361    data: 0.0354    max mem: 21\n",
      "Test: Epoch: [70] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1594 (0.1914)    accuracy: 0.9453 (0.9292)\n",
      "Epoch: [71]    [ 0/62]    eta: 0:00:06    lr: 0.000274    loss: 0.1595 (0.1595)    accuracy: 0.9453 (0.9453)    time: 0.1094    data: 0.0361    max mem: 21\n",
      "Epoch: [71] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000274    loss: 0.1497 (0.1443)    accuracy: 0.9512 (0.9516)\n",
      "Test: Epoch: [71]    [ 0/15]    eta: 0:00:00    loss: 0.1911 (0.1911)    accuracy: 0.9297 (0.9297)    time: 0.0356    data: 0.0349    max mem: 21\n",
      "Test: Epoch: [71] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1550 (0.1891)    accuracy: 0.9453 (0.9302)\n",
      "Epoch: [72]    [ 0/62]    eta: 0:00:06    lr: 0.000263    loss: 0.1280 (0.1280)    accuracy: 0.9512 (0.9512)    time: 0.1111    data: 0.0366    max mem: 21\n",
      "Epoch: [72] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000263    loss: 0.1585 (0.1440)    accuracy: 0.9375 (0.9515)\n",
      "Test: Epoch: [72]    [ 0/15]    eta: 0:00:00    loss: 0.1881 (0.1881)    accuracy: 0.9336 (0.9336)    time: 0.0359    data: 0.0352    max mem: 21\n",
      "Test: Epoch: [72] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1631 (0.1924)    accuracy: 0.9355 (0.9280)\n",
      "Epoch: [73]    [ 0/62]    eta: 0:00:06    lr: 0.000252    loss: 0.1359 (0.1359)    accuracy: 0.9492 (0.9492)    time: 0.1088    data: 0.0361    max mem: 21\n",
      "Epoch: [73] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000252    loss: 0.1544 (0.1440)    accuracy: 0.9434 (0.9520)\n",
      "Test: Epoch: [73]    [ 0/15]    eta: 0:00:00    loss: 0.1956 (0.1956)    accuracy: 0.9258 (0.9258)    time: 0.0356    data: 0.0349    max mem: 21\n",
      "Test: Epoch: [73] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1636 (0.1944)    accuracy: 0.9395 (0.9271)\n",
      "Epoch: [74]    [ 0/62]    eta: 0:00:06    lr: 0.000242    loss: 0.1506 (0.1506)    accuracy: 0.9395 (0.9395)    time: 0.1099    data: 0.0367    max mem: 21\n",
      "Epoch: [74] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000242    loss: 0.1460 (0.1436)    accuracy: 0.9434 (0.9510)\n",
      "Test: Epoch: [74]    [ 0/15]    eta: 0:00:00    loss: 0.1938 (0.1938)    accuracy: 0.9238 (0.9238)    time: 0.0357    data: 0.0350    max mem: 21\n",
      "Test: Epoch: [74] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1574 (0.1909)    accuracy: 0.9434 (0.9292)\n",
      "Epoch: [75]    [ 0/62]    eta: 0:00:06    lr: 0.000232    loss: 0.1570 (0.1570)    accuracy: 0.9414 (0.9414)    time: 0.1105    data: 0.0360    max mem: 21\n",
      "Epoch: [75] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000232    loss: 0.1127 (0.1419)    accuracy: 0.9648 (0.9516)\n",
      "Test: Epoch: [75]    [ 0/15]    eta: 0:00:00    loss: 0.1969 (0.1969)    accuracy: 0.9277 (0.9277)    time: 0.0362    data: 0.0355    max mem: 21\n",
      "Test: Epoch: [75] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1605 (0.1926)    accuracy: 0.9414 (0.9292)\n",
      "Epoch: [76]    [ 0/62]    eta: 0:00:06    lr: 0.000222    loss: 0.1583 (0.1583)    accuracy: 0.9492 (0.9492)    time: 0.1095    data: 0.0362    max mem: 21\n",
      "Epoch: [76] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000222    loss: 0.1364 (0.1415)    accuracy: 0.9590 (0.9513)\n",
      "Test: Epoch: [76]    [ 0/15]    eta: 0:00:00    loss: 0.1975 (0.1975)    accuracy: 0.9258 (0.9258)    time: 0.0400    data: 0.0393    max mem: 21\n",
      "Test: Epoch: [76] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1609 (0.1927)    accuracy: 0.9434 (0.9286)\n",
      "Epoch: [77]    [ 0/62]    eta: 0:00:06    lr: 0.000212    loss: 0.1262 (0.1262)    accuracy: 0.9570 (0.9570)    time: 0.1103    data: 0.0365    max mem: 21\n",
      "Epoch: [77] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000212    loss: 0.1566 (0.1412)    accuracy: 0.9414 (0.9511)\n",
      "Test: Epoch: [77]    [ 0/15]    eta: 0:00:00    loss: 0.1918 (0.1918)    accuracy: 0.9277 (0.9277)    time: 0.0357    data: 0.0350    max mem: 21\n",
      "Test: Epoch: [77] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1557 (0.1891)    accuracy: 0.9473 (0.9296)\n",
      "Epoch: [78]    [ 0/62]    eta: 0:00:06    lr: 0.000203    loss: 0.1605 (0.1605)    accuracy: 0.9492 (0.9492)    time: 0.1088    data: 0.0359    max mem: 21\n",
      "Epoch: [78] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000203    loss: 0.1407 (0.1419)    accuracy: 0.9492 (0.9519)\n",
      "Test: Epoch: [78]    [ 0/15]    eta: 0:00:00    loss: 0.1990 (0.1990)    accuracy: 0.9258 (0.9258)    time: 0.0389    data: 0.0382    max mem: 21\n",
      "Test: Epoch: [78] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1632 (0.1938)    accuracy: 0.9434 (0.9275)\n",
      "Epoch: [79]    [ 0/62]    eta: 0:00:06    lr: 0.000194    loss: 0.1467 (0.1467)    accuracy: 0.9473 (0.9473)    time: 0.1092    data: 0.0360    max mem: 21\n",
      "Epoch: [79] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000194    loss: 0.1331 (0.1416)    accuracy: 0.9531 (0.9516)\n",
      "Test: Epoch: [79]    [ 0/15]    eta: 0:00:00    loss: 0.1957 (0.1957)    accuracy: 0.9258 (0.9258)    time: 0.0403    data: 0.0396    max mem: 21\n",
      "Test: Epoch: [79] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1594 (0.1919)    accuracy: 0.9434 (0.9299)\n",
      "Epoch: [80]    [ 0/62]    eta: 0:00:06    lr: 0.000186    loss: 0.1429 (0.1429)    accuracy: 0.9531 (0.9531)    time: 0.1059    data: 0.0362    max mem: 21\n",
      "Epoch: [80] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000186    loss: 0.1338 (0.1407)    accuracy: 0.9570 (0.9518)\n",
      "Test: Epoch: [80]    [ 0/15]    eta: 0:00:00    loss: 0.1966 (0.1966)    accuracy: 0.9238 (0.9238)    time: 0.0368    data: 0.0361    max mem: 21\n",
      "Test: Epoch: [80] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1562 (0.1908)    accuracy: 0.9473 (0.9298)\n",
      "Epoch: [81]    [ 0/62]    eta: 0:00:06    lr: 0.000178    loss: 0.1301 (0.1301)    accuracy: 0.9551 (0.9551)    time: 0.1108    data: 0.0362    max mem: 21\n",
      "Epoch: [81] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000178    loss: 0.1136 (0.1400)    accuracy: 0.9668 (0.9518)\n",
      "Test: Epoch: [81]    [ 0/15]    eta: 0:00:00    loss: 0.1934 (0.1934)    accuracy: 0.9297 (0.9297)    time: 0.0391    data: 0.0384    max mem: 21\n",
      "Test: Epoch: [81] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1591 (0.1912)    accuracy: 0.9453 (0.9293)\n",
      "Epoch: [82]    [ 0/62]    eta: 0:00:06    lr: 0.000170    loss: 0.1370 (0.1370)    accuracy: 0.9395 (0.9395)    time: 0.1090    data: 0.0365    max mem: 21\n",
      "Epoch: [82] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000170    loss: 0.1430 (0.1398)    accuracy: 0.9453 (0.9514)\n",
      "Test: Epoch: [82]    [ 0/15]    eta: 0:00:00    loss: 0.1952 (0.1952)    accuracy: 0.9277 (0.9277)    time: 0.0362    data: 0.0354    max mem: 21\n",
      "Test: Epoch: [82] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1604 (0.1918)    accuracy: 0.9414 (0.9281)\n",
      "Epoch: [83]    [ 0/62]    eta: 0:00:06    lr: 0.000163    loss: 0.1252 (0.1252)    accuracy: 0.9570 (0.9570)    time: 0.1049    data: 0.0361    max mem: 21\n",
      "Epoch: [83] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000163    loss: 0.1291 (0.1389)    accuracy: 0.9609 (0.9516)\n",
      "Test: Epoch: [83]    [ 0/15]    eta: 0:00:00    loss: 0.1982 (0.1982)    accuracy: 0.9277 (0.9277)    time: 0.0395    data: 0.0388    max mem: 21\n",
      "Test: Epoch: [83] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1608 (0.1922)    accuracy: 0.9434 (0.9284)\n",
      "Epoch: [84]    [ 0/62]    eta: 0:00:06    lr: 0.000156    loss: 0.1290 (0.1290)    accuracy: 0.9648 (0.9648)    time: 0.1129    data: 0.0359    max mem: 21\n",
      "Epoch: [84] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000156    loss: 0.1498 (0.1397)    accuracy: 0.9434 (0.9520)\n",
      "Test: Epoch: [84]    [ 0/15]    eta: 0:00:00    loss: 0.1979 (0.1979)    accuracy: 0.9277 (0.9277)    time: 0.0379    data: 0.0372    max mem: 21\n",
      "Test: Epoch: [84] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1612 (0.1926)    accuracy: 0.9434 (0.9280)\n",
      "Epoch: [85]    [ 0/62]    eta: 0:00:06    lr: 0.000149    loss: 0.1246 (0.1246)    accuracy: 0.9590 (0.9590)    time: 0.1042    data: 0.0359    max mem: 21\n",
      "Epoch: [85] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000149    loss: 0.1228 (0.1390)    accuracy: 0.9531 (0.9517)\n",
      "Test: Epoch: [85]    [ 0/15]    eta: 0:00:00    loss: 0.1992 (0.1992)    accuracy: 0.9258 (0.9258)    time: 0.0354    data: 0.0347    max mem: 21\n",
      "Test: Epoch: [85] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1644 (0.1956)    accuracy: 0.9375 (0.9270)\n",
      "Epoch: [86]    [ 0/62]    eta: 0:00:06    lr: 0.000143    loss: 0.1332 (0.1332)    accuracy: 0.9590 (0.9590)    time: 0.1097    data: 0.0363    max mem: 21\n",
      "Epoch: [86] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000143    loss: 0.1276 (0.1389)    accuracy: 0.9609 (0.9513)\n",
      "Test: Epoch: [86]    [ 0/15]    eta: 0:00:00    loss: 0.1950 (0.1950)    accuracy: 0.9258 (0.9258)    time: 0.0385    data: 0.0377    max mem: 21\n",
      "Test: Epoch: [86] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1600 (0.1907)    accuracy: 0.9434 (0.9281)\n",
      "Epoch: [87]    [ 0/62]    eta: 0:00:06    lr: 0.000137    loss: 0.0938 (0.0938)    accuracy: 0.9668 (0.9668)    time: 0.1113    data: 0.0366    max mem: 21\n",
      "Epoch: [87] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000137    loss: 0.1026 (0.1384)    accuracy: 0.9648 (0.9517)\n",
      "Test: Epoch: [87]    [ 0/15]    eta: 0:00:00    loss: 0.1964 (0.1964)    accuracy: 0.9277 (0.9277)    time: 0.0355    data: 0.0348    max mem: 21\n",
      "Test: Epoch: [87] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1598 (0.1914)    accuracy: 0.9434 (0.9281)\n",
      "Epoch: [88]    [ 0/62]    eta: 0:00:06    lr: 0.000132    loss: 0.1342 (0.1342)    accuracy: 0.9512 (0.9512)    time: 0.1086    data: 0.0358    max mem: 21\n",
      "Epoch: [88] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000132    loss: 0.1287 (0.1379)    accuracy: 0.9512 (0.9521)\n",
      "Test: Epoch: [88]    [ 0/15]    eta: 0:00:00    loss: 0.1946 (0.1946)    accuracy: 0.9277 (0.9277)    time: 0.0358    data: 0.0351    max mem: 21\n",
      "Test: Epoch: [88] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1618 (0.1929)    accuracy: 0.9414 (0.9277)\n",
      "Epoch: [89]    [ 0/62]    eta: 0:00:06    lr: 0.000127    loss: 0.1426 (0.1426)    accuracy: 0.9453 (0.9453)    time: 0.1084    data: 0.0358    max mem: 21\n",
      "Epoch: [89] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000127    loss: 0.1937 (0.1378)    accuracy: 0.9434 (0.9517)\n",
      "Test: Epoch: [89]    [ 0/15]    eta: 0:00:00    loss: 0.1965 (0.1965)    accuracy: 0.9258 (0.9258)    time: 0.0356    data: 0.0349    max mem: 21\n",
      "Test: Epoch: [89] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1586 (0.1908)    accuracy: 0.9434 (0.9293)\n",
      "Epoch: [90]    [ 0/62]    eta: 0:00:06    lr: 0.000122    loss: 0.1308 (0.1308)    accuracy: 0.9531 (0.9531)    time: 0.1099    data: 0.0362    max mem: 21\n",
      "Epoch: [90] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000122    loss: 0.1174 (0.1376)    accuracy: 0.9512 (0.9521)\n",
      "Test: Epoch: [90]    [ 0/15]    eta: 0:00:00    loss: 0.1961 (0.1961)    accuracy: 0.9258 (0.9258)    time: 0.0360    data: 0.0353    max mem: 21\n",
      "Test: Epoch: [90] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1591 (0.1911)    accuracy: 0.9434 (0.9290)\n",
      "Epoch: [91]    [ 0/62]    eta: 0:00:06    lr: 0.000118    loss: 0.1151 (0.1151)    accuracy: 0.9551 (0.9551)    time: 0.1110    data: 0.0358    max mem: 21\n",
      "Epoch: [91] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000118    loss: 0.1233 (0.1371)    accuracy: 0.9531 (0.9516)\n",
      "Test: Epoch: [91]    [ 0/15]    eta: 0:00:00    loss: 0.1981 (0.1981)    accuracy: 0.9238 (0.9238)    time: 0.0356    data: 0.0348    max mem: 21\n",
      "Test: Epoch: [91] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1607 (0.1919)    accuracy: 0.9434 (0.9284)\n",
      "Epoch: [92]    [ 0/62]    eta: 0:00:06    lr: 0.000114    loss: 0.1438 (0.1438)    accuracy: 0.9590 (0.9590)    time: 0.1090    data: 0.0362    max mem: 21\n",
      "Epoch: [92] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000114    loss: 0.1087 (0.1370)    accuracy: 0.9648 (0.9517)\n",
      "Test: Epoch: [92]    [ 0/15]    eta: 0:00:00    loss: 0.1949 (0.1949)    accuracy: 0.9258 (0.9258)    time: 0.0386    data: 0.0379    max mem: 21\n",
      "Test: Epoch: [92] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1625 (0.1936)    accuracy: 0.9434 (0.9279)\n",
      "Epoch: [93]    [ 0/62]    eta: 0:00:06    lr: 0.000111    loss: 0.0989 (0.0989)    accuracy: 0.9707 (0.9707)    time: 0.1109    data: 0.0379    max mem: 21\n",
      "Epoch: [93] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000111    loss: 0.1711 (0.1369)    accuracy: 0.9395 (0.9515)\n",
      "Test: Epoch: [93]    [ 0/15]    eta: 0:00:00    loss: 0.1967 (0.1967)    accuracy: 0.9277 (0.9277)    time: 0.0394    data: 0.0386    max mem: 21\n",
      "Test: Epoch: [93] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1584 (0.1910)    accuracy: 0.9473 (0.9283)\n",
      "Epoch: [94]    [ 0/62]    eta: 0:00:07    lr: 0.000108    loss: 0.1352 (0.1352)    accuracy: 0.9512 (0.9512)    time: 0.1162    data: 0.0402    max mem: 21\n",
      "Epoch: [94] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000108    loss: 0.1250 (0.1368)    accuracy: 0.9570 (0.9515)\n",
      "Test: Epoch: [94]    [ 0/15]    eta: 0:00:00    loss: 0.1953 (0.1953)    accuracy: 0.9277 (0.9277)    time: 0.0355    data: 0.0348    max mem: 21\n",
      "Test: Epoch: [94] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1583 (0.1907)    accuracy: 0.9453 (0.9286)\n",
      "Epoch: [95]    [ 0/62]    eta: 0:00:06    lr: 0.000106    loss: 0.1573 (0.1573)    accuracy: 0.9473 (0.9473)    time: 0.1084    data: 0.0360    max mem: 21\n",
      "Epoch: [95] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000106    loss: 0.1277 (0.1362)    accuracy: 0.9590 (0.9520)\n",
      "Test: Epoch: [95]    [ 0/15]    eta: 0:00:00    loss: 0.2020 (0.2020)    accuracy: 0.9277 (0.9277)    time: 0.0355    data: 0.0348    max mem: 21\n",
      "Test: Epoch: [95] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1620 (0.1940)    accuracy: 0.9434 (0.9276)\n",
      "Epoch: [96]    [ 0/62]    eta: 0:00:06    lr: 0.000104    loss: 0.1313 (0.1313)    accuracy: 0.9531 (0.9531)    time: 0.1090    data: 0.0360    max mem: 21\n",
      "Epoch: [96] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000104    loss: 0.1485 (0.1363)    accuracy: 0.9492 (0.9515)\n",
      "Test: Epoch: [96]    [ 0/15]    eta: 0:00:00    loss: 0.1970 (0.1970)    accuracy: 0.9258 (0.9258)    time: 0.0358    data: 0.0351    max mem: 21\n",
      "Test: Epoch: [96] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1593 (0.1924)    accuracy: 0.9453 (0.9283)\n",
      "Epoch: [97]    [ 0/62]    eta: 0:00:06    lr: 0.000102    loss: 0.1316 (0.1316)    accuracy: 0.9570 (0.9570)    time: 0.1096    data: 0.0362    max mem: 21\n",
      "Epoch: [97] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000102    loss: 0.1230 (0.1365)    accuracy: 0.9609 (0.9515)\n",
      "Test: Epoch: [97]    [ 0/15]    eta: 0:00:00    loss: 0.1949 (0.1949)    accuracy: 0.9277 (0.9277)    time: 0.0354    data: 0.0347    max mem: 21\n",
      "Test: Epoch: [97] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1586 (0.1914)    accuracy: 0.9453 (0.9301)\n",
      "Epoch: [98]    [ 0/62]    eta: 0:00:06    lr: 0.000101    loss: 0.1676 (0.1676)    accuracy: 0.9414 (0.9414)    time: 0.1093    data: 0.0363    max mem: 21\n",
      "Epoch: [98] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000101    loss: 0.1123 (0.1360)    accuracy: 0.9629 (0.9516)\n",
      "Test: Epoch: [98]    [ 0/15]    eta: 0:00:00    loss: 0.1986 (0.1986)    accuracy: 0.9297 (0.9297)    time: 0.0354    data: 0.0346    max mem: 21\n",
      "Test: Epoch: [98] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1616 (0.1923)    accuracy: 0.9434 (0.9302)\n",
      "Epoch: [99]    [ 0/62]    eta: 0:00:06    lr: 0.000100    loss: 0.1512 (0.1512)    accuracy: 0.9512 (0.9512)    time: 0.1079    data: 0.0358    max mem: 21\n",
      "Epoch: [99] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.000100    loss: 0.1332 (0.1364)    accuracy: 0.9629 (0.9517)\n",
      "Test: Epoch: [99]    [ 0/15]    eta: 0:00:00    loss: 0.1942 (0.1942)    accuracy: 0.9277 (0.9277)    time: 0.0362    data: 0.0355    max mem: 21\n",
      "Test: Epoch: [99] Total time: 0:00:00\n",
      "Averaged stats: loss: 0.1582 (0.1908)    accuracy: 0.9453 (0.9284)\n",
      "Total time: 675s\n",
      "Best loss: 0.18479526440302532 ;  Best Epoch: 45\n",
      "Min of model parameters: -0.4499501883983612, Max of model parameters: 0.44976893067359924\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjAAAAGdCAYAAAAMm0nCAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuNSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/xnp5ZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAqG0lEQVR4nO3dfXRU9Z3H8U9CmAlPk/BgZkwJkC4tEAsqUGGqWMAsIx09sKbdghSpRlk4wS7JFjDncIBit1AqAmqAVZDQUzk87CmuECSE50WGByNpIyjrA26oMJO2mAxQSAi5+0dP7jII6ISE5Bfer3N+5zD3972/+V4uMR9v7s3EWJZlCQAAwCCxTd0AAABAtAgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjxDV1A42ltrZWp06dUocOHRQTE9PU7QAAgK/BsiydPXtWycnJio29/nWWFhtgTp06pZSUlKZuAwAA1MPJkyfVtWvX68632ADToUMHSX//C3C5XE3cDQAA+DrC4bBSUlLs7+PX02IDTN2PjVwuFwEGAADDfNXtH9zECwAAjEOAAQAAxokqwPTo0UMxMTFfGllZWZKkixcvKisrS507d1b79u2VkZGhUCgUsUZZWZn8fr/atm2rpKQkTZs2TTU1NRE1u3fvVv/+/eV0OtWzZ0/l5+ff3FECAIAWJaoAc/jwYZ0+fdoeRUVFkqQf/ehHkqTs7Gxt2rRJGzZs0J49e3Tq1Ck99thj9v6XL1+W3+9XdXW19u/fr9WrVys/P1+zZs2ya06cOCG/369hw4appKREU6dO1dNPP63CwsKGOF4AANACxFiWZdV356lTp2rz5s366KOPFA6Hdccdd2jNmjX64Q9/KEn68MMP1adPHwUCAQ0ePFhvv/22HnnkEZ06dUput1uStHz5cs2YMUN//vOf5XA4NGPGDBUUFOj999+332fMmDGqqKjQ1q1bv3Zv4XBYCQkJqqys5CZeAAAM8XW/f9f7Hpjq6mr97ne/01NPPaWYmBgVFxfr0qVLSk9Pt2t69+6tbt26KRAISJICgYD69u1rhxdJ8vl8CofDOnr0qF1z5Rp1NXVrXE9VVZXC4XDEAAAALVO9A8ybb76piooK/fSnP5UkBYNBORwOJSYmRtS53W4Fg0G75srwUjdfN3ejmnA4rAsXLly3n3nz5ikhIcEe/BI7AABarnoHmJUrV2rkyJFKTk5uyH7qLTc3V5WVlfY4efJkU7cEAAAaSb1+kd3//u//avv27fr9739vb/N4PKqurlZFRUXEVZhQKCSPx2PXHDp0KGKtuqeUrqy5+smlUCgkl8ulNm3aXLcnp9Mpp9NZn8MBAACGqdcVmFWrVikpKUl+v9/eNmDAALVu3Vo7duywtx0/flxlZWXyer2SJK/Xq9LSUpWXl9s1RUVFcrlcSktLs2uuXKOupm4NAACAqANMbW2tVq1apQkTJigu7v8v4CQkJCgzM1M5OTnatWuXiouL9eSTT8rr9Wrw4MGSpBEjRigtLU3jx4/XH/7wBxUWFmrmzJnKysqyr55MmjRJn376qaZPn64PP/xQS5cu1fr165Wdnd1AhwwAAEwX9Y+Qtm/frrKyMj311FNfmlu0aJFiY2OVkZGhqqoq+Xw+LV261J5v1aqVNm/erMmTJ8vr9apdu3aaMGGC5s6da9ekpqaqoKBA2dnZWrJkibp27aoVK1bI5/PV8xABAEBLc1O/B6Y54/fAAABgnkb/PTAAAABNpV5PIQFAU+vxXEHE68/m+69TCaAl4goMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA48Q1dQMAWo4ezxVEvP5svr+JOgHQ0nEFBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcXgKCQCucPWTVBJPUwHNEVdgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4UQeYzz//XD/5yU/UuXNntWnTRn379tW7775rz1uWpVmzZunOO+9UmzZtlJ6ero8++ihijTNnzmjcuHFyuVxKTExUZmamzp07F1Hzxz/+UUOGDFF8fLxSUlK0YMGCeh4iAABoaaIKMF988YXuv/9+tW7dWm+//baOHTumhQsXqmPHjnbNggUL9NJLL2n58uU6ePCg2rVrJ5/Pp4sXL9o148aN09GjR1VUVKTNmzdr7969mjhxoj0fDoc1YsQIde/eXcXFxfrNb36jOXPm6NVXX22AQwYAAKaL6sMcf/3rXyslJUWrVq2yt6Wmptp/tixLixcv1syZMzVq1ChJ0m9/+1u53W69+eabGjNmjD744ANt3bpVhw8f1sCBAyVJL7/8sn7wgx/ohRdeUHJyst544w1VV1fr9ddfl8Ph0F133aWSkhK9+OKLEUEHAADcnqK6AvPWW29p4MCB+tGPfqSkpCTde++9eu211+z5EydOKBgMKj093d6WkJCgQYMGKRAISJICgYASExPt8CJJ6enpio2N1cGDB+2aBx98UA6Hw67x+Xw6fvy4vvjii2v2VlVVpXA4HDEAAEDLFFWA+fTTT7Vs2TJ961vfUmFhoSZPnqyf/exnWr16tSQpGAxKktxud8R+brfbngsGg0pKSoqYj4uLU6dOnSJqrrXGle9xtXnz5ikhIcEeKSkp0RwaAAAwSFQBpra2Vv3799evfvUr3XvvvZo4caKeeeYZLV++vLH6+9pyc3NVWVlpj5MnTzZ1SwAAoJFEFWDuvPNOpaWlRWzr06ePysrKJEkej0eSFAqFImpCoZA95/F4VF5eHjFfU1OjM2fORNRca40r3+NqTqdTLpcrYgAAgJYpqgBz//336/jx4xHb/ud//kfdu3eX9Pcbej0ej3bs2GHPh8NhHTx4UF6vV5Lk9XpVUVGh4uJiu2bnzp2qra3VoEGD7Jq9e/fq0qVLdk1RUZF69eoV8cQTAAC4PUUVYLKzs3XgwAH96le/0scff6w1a9bo1VdfVVZWliQpJiZGU6dO1S9/+Uu99dZbKi0t1RNPPKHk5GSNHj1a0t+v2Dz88MN65plndOjQIb3zzjuaMmWKxowZo+TkZEnS448/LofDoczMTB09elTr1q3TkiVLlJOT07BHDwAAjBTVY9Tf/e53tXHjRuXm5mru3LlKTU3V4sWLNW7cOLtm+vTpOn/+vCZOnKiKigo98MAD2rp1q+Lj4+2aN954Q1OmTNFDDz2k2NhYZWRk6KWXXrLnExIStG3bNmVlZWnAgAHq0qWLZs2axSPUAABAkhRjWZbV1E00hnA4rISEBFVWVnI/DHCL9HiuIOL1Z/P9xr3X1es25NoAvtrX/f7NZyEBAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA40QVYObMmaOYmJiI0bt3b3v+4sWLysrKUufOndW+fXtlZGQoFApFrFFWVia/36+2bdsqKSlJ06ZNU01NTUTN7t271b9/fzmdTvXs2VP5+fn1P0IAANDiRH0F5q677tLp06ftsW/fPnsuOztbmzZt0oYNG7Rnzx6dOnVKjz32mD1/+fJl+f1+VVdXa//+/Vq9erXy8/M1a9Ysu+bEiRPy+/0aNmyYSkpKNHXqVD399NMqLCy8yUMFAAAtRVzUO8TFyePxfGl7ZWWlVq5cqTVr1mj48OGSpFWrVqlPnz46cOCABg8erG3btunYsWPavn273G637rnnHj3//POaMWOG5syZI4fDoeXLlys1NVULFy6UJPXp00f79u3TokWL5PP5bvJwAQBASxD1FZiPPvpIycnJ+uY3v6lx48aprKxMklRcXKxLly4pPT3dru3du7e6deumQCAgSQoEAurbt6/cbrdd4/P5FA6HdfToUbvmyjXqaurWuJ6qqiqFw+GIAQAAWqaoAsygQYOUn5+vrVu3atmyZTpx4oSGDBmis2fPKhgMyuFwKDExMWIft9utYDAoSQoGgxHhpW6+bu5GNeFwWBcuXLhub/PmzVNCQoI9UlJSojk0AABgkKh+hDRy5Ej7z/369dOgQYPUvXt3rV+/Xm3atGnw5qKRm5urnJwc+3U4HCbEAADQQt3UY9SJiYn69re/rY8//lgej0fV1dWqqKiIqAmFQvY9Mx6P50tPJdW9/qoal8t1w5DkdDrlcrkiBgAAaJluKsCcO3dOn3zyie68804NGDBArVu31o4dO+z548ePq6ysTF6vV5Lk9XpVWlqq8vJyu6aoqEgul0tpaWl2zZVr1NXUrQEAABBVgPn5z3+uPXv26LPPPtP+/fv1T//0T2rVqpXGjh2rhIQEZWZmKicnR7t27VJxcbGefPJJeb1eDR48WJI0YsQIpaWlafz48frDH/6gwsJCzZw5U1lZWXI6nZKkSZMm6dNPP9X06dP14YcfaunSpVq/fr2ys7Mb/ugBAICRoroH5k9/+pPGjh2rv/71r7rjjjv0wAMP6MCBA7rjjjskSYsWLVJsbKwyMjJUVVUln8+npUuX2vu3atVKmzdv1uTJk+X1etWuXTtNmDBBc+fOtWtSU1NVUFCg7OxsLVmyRF27dtWKFSt4hBoAANiiCjBr16694Xx8fLzy8vKUl5d33Zru3btry5YtN1xn6NChOnLkSDStAQCA2wifhQQAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAY56YCzPz58xUTE6OpU6fa2y5evKisrCx17txZ7du3V0ZGhkKhUMR+ZWVl8vv9atu2rZKSkjRt2jTV1NRE1OzevVv9+/eX0+lUz549lZ+ffzOtAgCAFqTeAebw4cP6j//4D/Xr1y9ie3Z2tjZt2qQNGzZoz549OnXqlB577DF7/vLly/L7/aqurtb+/fu1evVq5efna9asWXbNiRMn5Pf7NWzYMJWUlGjq1Kl6+umnVVhYWN92AQBAC1KvAHPu3DmNGzdOr732mjp27Ghvr6ys1MqVK/Xiiy9q+PDhGjBggFatWqX9+/frwIEDkqRt27bp2LFj+t3vfqd77rlHI0eO1PPPP6+8vDxVV1dLkpYvX67U1FQtXLhQffr00ZQpU/TDH/5QixYtaoBDBgAApqtXgMnKypLf71d6enrE9uLiYl26dClie+/evdWtWzcFAgFJUiAQUN++feV2u+0an8+ncDiso0eP2jVXr+3z+ew1rqWqqkrhcDhiAACAliku2h3Wrl2r9957T4cPH/7SXDAYlMPhUGJiYsR2t9utYDBo11wZXurm6+ZuVBMOh3XhwgW1adPmS+89b948/eIXv4j2cAAAgIGiugJz8uRJ/eu//qveeOMNxcfHN1ZP9ZKbm6vKykp7nDx5sqlbAgAAjSSqAFNcXKzy8nL1799fcXFxiouL0549e/TSSy8pLi5Obrdb1dXVqqioiNgvFArJ4/FIkjwez5eeSqp7/VU1LpfrmldfJMnpdMrlckUMAADQMkUVYB566CGVlpaqpKTEHgMHDtS4cePsP7du3Vo7duyw9zl+/LjKysrk9XolSV6vV6WlpSovL7drioqK5HK5lJaWZtdcuUZdTd0aAADg9hbVPTAdOnTQd77znYht7dq1U+fOne3tmZmZysnJUadOneRyufTss8/K6/Vq8ODBkqQRI0YoLS1N48eP14IFCxQMBjVz5kxlZWXJ6XRKkiZNmqRXXnlF06dP11NPPaWdO3dq/fr1KigoaIhjBgAAhov6Jt6vsmjRIsXGxiojI0NVVVXy+XxaunSpPd+qVStt3rxZkydPltfrVbt27TRhwgTNnTvXrklNTVVBQYGys7O1ZMkSde3aVStWrJDP52vodgEAgIFuOsDs3r074nV8fLzy8vKUl5d33X26d++uLVu23HDdoUOH6siRIzfbHgAAaIH4LCQAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHHimroBAI2rx3MFEa8/m+9vok4AoOEQYACgiVwdLiUCJvB18SMkAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4UQWYZcuWqV+/fnK5XHK5XPJ6vXr77bft+YsXLyorK0udO3dW+/btlZGRoVAoFLFGWVmZ/H6/2rZtq6SkJE2bNk01NTURNbt371b//v3ldDrVs2dP5efn1/8IAQBAixNVgOnatavmz5+v4uJivfvuuxo+fLhGjRqlo0ePSpKys7O1adMmbdiwQXv27NGpU6f02GOP2ftfvnxZfr9f1dXV2r9/v1avXq38/HzNmjXLrjlx4oT8fr+GDRumkpISTZ06VU8//bQKCwsb6JABAIDpovoogUcffTTi9b//+79r2bJlOnDggLp27aqVK1dqzZo1Gj58uCRp1apV6tOnjw4cOKDBgwdr27ZtOnbsmLZv3y6326177rlHzz//vGbMmKE5c+bI4XBo+fLlSk1N1cKFCyVJffr00b59+7Ro0SL5fL4GOmwAAGCyet8Dc/nyZa1du1bnz5+X1+tVcXGxLl26pPT0dLumd+/e6tatmwKBgCQpEAiob9++crvddo3P51M4HLav4gQCgYg16mrq1rieqqoqhcPhiAEAAFqmqANMaWmp2rdvL6fTqUmTJmnjxo1KS0tTMBiUw+FQYmJiRL3b7VYwGJQkBYPBiPBSN183d6OacDisCxcuXLevefPmKSEhwR4pKSnRHhoAADBE1AGmV69eKikp0cGDBzV58mRNmDBBx44da4zeopKbm6vKykp7nDx5sqlbAgAAjSSqe2AkyeFwqGfPnpKkAQMG6PDhw1qyZIl+/OMfq7q6WhUVFRFXYUKhkDwejyTJ4/Ho0KFDEevVPaV0Zc3VTy6FQiG5XC61adPmun05nU45nc5oDwcAABjopn8PTG1traqqqjRgwAC1bt1aO3bssOeOHz+usrIyeb1eSZLX61VpaanKy8vtmqKiIrlcLqWlpdk1V65RV1O3BgAAQFRXYHJzczVy5Eh169ZNZ8+e1Zo1a7R7924VFhYqISFBmZmZysnJUadOneRyufTss8/K6/Vq8ODBkqQRI0YoLS1N48eP14IFCxQMBjVz5kxlZWXZV08mTZqkV155RdOnT9dTTz2lnTt3av369SooKGj4owcAAEaKKsCUl5friSee0OnTp5WQkKB+/fqpsLBQ//iP/yhJWrRokWJjY5WRkaGqqir5fD4tXbrU3r9Vq1bavHmzJk+eLK/Xq3bt2mnChAmaO3euXZOamqqCggJlZ2dryZIl6tq1q1asWMEj1AAAwBZVgFm5cuUN5+Pj45WXl6e8vLzr1nTv3l1btmy54TpDhw7VkSNHomkNAADcRvgsJAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4UQWYefPm6bvf/a46dOigpKQkjR49WsePH4+ouXjxorKystS5c2e1b99eGRkZCoVCETVlZWXy+/1q27atkpKSNG3aNNXU1ETU7N69W/3795fT6VTPnj2Vn59fvyMEAAAtTlQBZs+ePcrKytKBAwdUVFSkS5cuacSIETp//rxdk52drU2bNmnDhg3as2ePTp06pccee8yev3z5svx+v6qrq7V//36tXr1a+fn5mjVrll1z4sQJ+f1+DRs2TCUlJZo6daqefvppFRYWNsAhAwAA08VFU7x169aI1/n5+UpKSlJxcbEefPBBVVZWauXKlVqzZo2GDx8uSVq1apX69OmjAwcOaPDgwdq2bZuOHTum7du3y+1265577tHzzz+vGTNmaM6cOXI4HFq+fLlSU1O1cOFCSVKfPn20b98+LVq0SD6fr4EOHQAAmOqm7oGprKyUJHXq1EmSVFxcrEuXLik9Pd2u6d27t7p166ZAICBJCgQC6tu3r9xut13j8/kUDod19OhRu+bKNepq6ta4lqqqKoXD4YgBAABapnoHmNraWk2dOlX333+/vvOd70iSgsGgHA6HEhMTI2rdbreCwaBdc2V4qZuvm7tRTTgc1oULF67Zz7x585SQkGCPlJSU+h4aAABo5uodYLKysvT+++9r7dq1DdlPveXm5qqystIeJ0+ebOqWAABAI4nqHpg6U6ZM0ebNm7V371517drV3u7xeFRdXa2KioqIqzChUEgej8euOXToUMR6dU8pXVlz9ZNLoVBILpdLbdq0uWZPTqdTTqezPocDAAAME9UVGMuyNGXKFG3cuFE7d+5UampqxPyAAQPUunVr7dixw952/PhxlZWVyev1SpK8Xq9KS0tVXl5u1xQVFcnlciktLc2uuXKNupq6NQAAwO0tqiswWVlZWrNmjf7rv/5LHTp0sO9ZSUhIUJs2bZSQkKDMzEzl5OSoU6dOcrlcevbZZ+X1ejV48GBJ0ogRI5SWlqbx48drwYIFCgaDmjlzprKysuwrKJMmTdIrr7yi6dOn66mnntLOnTu1fv16FRQUNPDhAwAAE0V1BWbZsmWqrKzU0KFDdeedd9pj3bp1ds2iRYv0yCOPKCMjQw8++KA8Ho9+//vf2/OtWrXS5s2b1apVK3m9Xv3kJz/RE088oblz59o1qampKigoUFFRke6++24tXLhQK1as4BFqAAAgKcorMJZlfWVNfHy88vLylJeXd92a7t27a8uWLTdcZ+jQoTpy5Eg07QEAgNsEn4UEAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwTlxTNwBA6vFcwZe2fTbf3wSdAIAZuAIDAACMQ4ABAADGIcAAAADjEGAAAIBxuIkXAFogbgxHS8cVGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACME3WA2bt3rx599FElJycrJiZGb775ZsS8ZVmaNWuW7rzzTrVp00bp6en66KOPImrOnDmjcePGyeVyKTExUZmZmTp37lxEzR//+EcNGTJE8fHxSklJ0YIFC6I/OgAA0CJFHWDOnz+vu+++W3l5edecX7BggV566SUtX75cBw8eVLt27eTz+XTx4kW7Zty4cTp69KiKioq0efNm7d27VxMnTrTnw+GwRowYoe7du6u4uFi/+c1vNGfOHL366qv1OEQAANDSxEW7w8iRIzVy5MhrzlmWpcWLF2vmzJkaNWqUJOm3v/2t3G633nzzTY0ZM0YffPCBtm7dqsOHD2vgwIGSpJdfflk/+MEP9MILLyg5OVlvvPGGqqur9frrr8vhcOiuu+5SSUmJXnzxxYigAwAAbk8Neg/MiRMnFAwGlZ6ebm9LSEjQoEGDFAgEJEmBQECJiYl2eJGk9PR0xcbG6uDBg3bNgw8+KIfDYdf4fD4dP35cX3zxxTXfu6qqSuFwOGIAAICWqUEDTDAYlCS53e6I7W63254LBoNKSkqKmI+Li1OnTp0iaq61xpXvcbV58+YpISHBHikpKTd/QAAAoFlqMU8h5ebmqrKy0h4nT55s6pYAAEAjadAA4/F4JEmhUChieygUsuc8Ho/Ky8sj5mtqanTmzJmImmutceV7XM3pdMrlckUMAADQMjVogElNTZXH49GOHTvsbeFwWAcPHpTX65Ukeb1eVVRUqLi42K7ZuXOnamtrNWjQILtm7969unTpkl1TVFSkXr16qWPHjg3ZMgAAMFDUAebcuXMqKSlRSUmJpL/fuFtSUqKysjLFxMRo6tSp+uUvf6m33npLpaWleuKJJ5ScnKzRo0dLkvr06aOHH35YzzzzjA4dOqR33nlHU6ZM0ZgxY5ScnCxJevzxx+VwOJSZmamjR49q3bp1WrJkiXJychrswAEAgLmifoz63Xff1bBhw+zXdaFiwoQJys/P1/Tp03X+/HlNnDhRFRUVeuCBB7R161bFx8fb+7zxxhuaMmWKHnroIcXGxiojI0MvvfSSPZ+QkKBt27YpKytLAwYMUJcuXTRr1iweoQYAAJLqEWCGDh0qy7KuOx8TE6O5c+dq7ty5163p1KmT1qxZc8P36devn/77v/872vYAAMBtoMU8hQQAAG4fBBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDgEGAAAYBwCDAAAMA4BBgAAGIcAAwAAjEOAAQAAxiHAAAAA4xBgAACAcQgwAADAOAQYAABgHAIMAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMYhwAAAAOMQYAAAgHEIMAAAwDhxTd0AAMBsPZ4r+NK2z+b7m6AT3E64AgMAAIzDFRjgOvi/SgBovrgCAwAAjEOAAQAAxiHAAAAA4xBgAACAcZp1gMnLy1OPHj0UHx+vQYMG6dChQ03dEgAAaAaabYBZt26dcnJyNHv2bL333nu6++675fP5VF5e3tStAQCAJtZsH6N+8cUX9cwzz+jJJ5+UJC1fvlwFBQV6/fXX9dxzzzVxdwBupWs90g7g9tYsA0x1dbWKi4uVm5trb4uNjVV6eroCgcA196mqqlJVVZX9urKyUpIUDocbt1k0ue/MLox4/f4vfA2ybm3V3760rbH+PTXme129dmN+TTTWe13r7+dqjfleLeG8N+TaTfleaPnq/u1YlnXjQqsZ+vzzzy1J1v79+yO2T5s2zbrvvvuuuc/s2bMtSQwGg8FgMFrAOHny5A2zQrO8AlMfubm5ysnJsV/X1tbqzJkz6ty5s2JiYpqws+iFw2GlpKTo5MmTcrlcTd0OxDlpbjgfzQvno3kx/XxYlqWzZ88qOTn5hnXNMsB06dJFrVq1UigUitgeCoXk8XiuuY/T6ZTT6YzYlpiY2Fgt3hIul8vIf3wtGeekeeF8NC+cj+bF5PORkJDwlTXN8ikkh8OhAQMGaMeOHfa22tpa7dixQ16vtwk7AwAAzUGzvAIjSTk5OZowYYIGDhyo++67T4sXL9b58+ftp5IAAMDtq9kGmB//+Mf685//rFmzZikYDOqee+7R1q1b5Xa7m7q1Rud0OjV79uwv/UgMTYdz0rxwPpoXzkfzcrucjxjL+qrnlAAAAJqXZnkPDAAAwI0QYAAAgHEIMAAAwDgEGAAAYBwCTDNx5swZjRs3Ti6XS4mJicrMzNS5c+e+1r6WZWnkyJGKiYnRm2++2biN3iaiPR9nzpzRs88+q169eqlNmzbq1q2bfvazn9mfyYXo5eXlqUePHoqPj9egQYN06NChG9Zv2LBBvXv3Vnx8vPr27astW7bcok5vD9Gcj9dee01DhgxRx44d1bFjR6Wnp3/l+UN0ov36qLN27VrFxMRo9OjRjdvgLUCAaSbGjRuno0ePqqioSJs3b9bevXs1ceLEr7Xv4sWLjfu4hOYu2vNx6tQpnTp1Si+88ILef/995efna+vWrcrMzLyFXbcc69atU05OjmbPnq333ntPd999t3w+n8rLy69Zv3//fo0dO1aZmZk6cuSIRo8erdGjR+v999+/xZ23TNGej927d2vs2LHatWuXAoGAUlJSNGLECH3++ee3uPOWKdrzUeezzz7Tz3/+cw0ZMuQWddrIGuTTF3FTjh07ZkmyDh8+bG97++23rZiYGOvzzz+/4b5HjhyxvvGNb1inT5+2JFkbN25s5G5bvps5H1dav3695XA4rEuXLjVGmy3afffdZ2VlZdmvL1++bCUnJ1vz5s27Zv0///M/W36/P2LboEGDrH/5l39p1D5vF9Gej6vV1NRYHTp0sFavXt1YLd5W6nM+ampqrO9973vWihUrrAkTJlijRo26BZ02Lq7ANAOBQECJiYkaOHCgvS09PV2xsbE6ePDgdff729/+pscff1x5eXnX/YwoRK++5+NqlZWVcrlciotrtr8vslmqrq5WcXGx0tPT7W2xsbFKT09XIBC45j6BQCCiXpJ8Pt916/H11ed8XO1vf/ubLl26pE6dOjVWm7eN+p6PuXPnKikpqUVdFea/rM1AMBhUUlJSxLa4uDh16tRJwWDwuvtlZ2fre9/7nkaNGtXYLd5W6ns+rvSXv/xFzz///Nf+MSD+31/+8hddvnz5S7912+1268MPP7zmPsFg8Jr1X/d84frqcz6uNmPGDCUnJ38pZCJ69Tkf+/bt08qVK1VSUnILOrx1uALTiJ577jnFxMTccHzd/wBc7a233tLOnTu1ePHihm26BWvM83GlcDgsv9+vtLQ0zZkz5+YbBww2f/58rV27Vhs3blR8fHxTt3PbOXv2rMaPH6/XXntNXbp0aep2GhRXYBrRv/3bv+mnP/3pDWu++c1vyuPxfOnmq5qaGp05c+a6PxrauXOnPvnkEyUmJkZsz8jI0JAhQ7R79+6b6LxlaszzUefs2bN6+OGH1aFDB23cuFGtW7e+2bZvO126dFGrVq0UCoUitodCoev+/Xs8nqjq8fXV53zUeeGFFzR//nxt375d/fr1a8w2bxvRno9PPvlEn332mR599FF7W21traS/X1k+fvy4/uEf/qFxm24sTX0TDv7/ptF3333X3lZYWHjDm0ZPnz5tlZaWRgxJ1pIlS6xPP/30VrXeItXnfFiWZVVWVlqDBw+2vv/971vnz5+/Fa22WPfdd581ZcoU+/Xly5etb3zjGze8ifeRRx6J2Ob1ermJt4FEez4sy7J+/etfWy6XywoEAreixdtKNOfjwoULX/peMWrUKGv48OFWaWmpVVVVdStbb1AEmGbi4Ycftu69917r4MGD1r59+6xvfetb1tixY+35P/3pT1avXr2sgwcPXncN8RRSg4n2fFRWVlqDBg2y+vbta3388cfW6dOn7VFTU9NUh2GstWvXWk6n08rPz7eOHTtmTZw40UpMTLSCwaBlWZY1fvx467nnnrPr33nnHSsuLs564YUXrA8++MCaPXu21bp1a6u0tLSpDqFFifZ8zJ8/33I4HNZ//ud/RnwtnD17tqkOoUWJ9nxcraU8hUSAaSb++te/WmPHjrXat29vuVwu68knn4z4Yj9x4oQlydq1a9d11yDANJxoz8euXbssSdccJ06caJqDMNzLL79sdevWzXI4HNZ9991nHThwwJ77/ve/b02YMCGifv369da3v/1ty+FwWHfddZdVUFBwiztu2aI5H927d7/m18Ls2bNvfeMtVLRfH1dqKQEmxrIs61b/2AoAAOBm8BQSAAAwDgEGAAAYhwADAACMQ4ABAADGIcAAAADjEGAAAIBxCDAAAMA4BBgAAGAcAgwAADAOAQYAABiHAAMAAIxDgAEAAMb5P623W1o2vay/AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(snr, mode, cnn, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b33865de-23bb-447d-9d6e-48081eb61fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "out_size = 4 if mode != 'qam16' else 16\n",
    "\n",
    "model = Detector_CNN(output_size=out_size).to(device) if cnn else Detector_MLP(output_size=out_size).to(device)\n",
    "model_path = \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/best_detection_model.pth\"\n",
    "ckpt = torch.load(model_path)['model']\n",
    "# for n, p in ckpt.items():\n",
    "#     print(n, p.shape)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 8, 16).to(device)\n",
    "torch.onnx.export(model, dummy_input,\n",
    "                  \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/symbol_detection.onnx\",\n",
    "                  export_params=True, opset_version=16, dynamic_axes={'input': {0: 'batch_size'}, 'sensor_out': {0: 'batch_size'}},\n",
    "                  input_names=['input'], output_names=['sensor_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed7542f-b4d2-4329-8db3-d3de0d4b9f96",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
