{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eaded07d-d196-4c0d-9a90-ddaaba117169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import os\n",
    "import scipy.io as sio\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "import torch.distributed as dist\n",
    "import errno\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a09addd0-98ac-478b-a2ab-6a97d67e1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{value:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {}'.format(header, total_time_str))\n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "    except OSError as e:\n",
    "        if e.errno != errno.EEXIST:\n",
    "            raise\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f33a748a-e0d2-423a-ba52-d0c37f88fefd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform\n",
    "def transform_data():\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "# create dataset class\n",
    "class channel_dataset(data.Dataset):\n",
    "    def __init__(self, data_path, use_noise: bool = False, is_train: bool = True):\n",
    "        super(channel_dataset, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        self.data = sio.loadmat(data_path)\n",
    "        self.use_noise = use_noise\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.trans = transform_data()\n",
    "\n",
    "        self.signals = self.data['filter_outputs'][:80000, :] if is_train else self.data['filter_outputs']\n",
    "        self.symbols = self.data['qam_symbols'][:, :80000] if is_train else self.data['qam_symbols']\n",
    "        self.symbols = np.reshape(self.symbols, (-1))\n",
    "\n",
    "        if use_noise:\n",
    "            noise = np.random.randint(-2, 3, size=self.signals.shape).astype(np.uint8)\n",
    "            self.signals += noise\n",
    "\n",
    "        self.signals = np.reshape(self.signals, (-1, 8, 16, 1)).astype(np.uint8)\n",
    "\n",
    "        self.num_samples = self.signals.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.symbols[index]).to(torch.long), self.trans(self.signals[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b5e754f7-922f-41d1-ad12-20ab0c026214",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector_CNN(nn.Module):\n",
    "    def __init__(self, output_size: int = 4):\n",
    "        super(Detector_CNN, self).__init__()\n",
    "        self.output_size = output_size\n",
    "\n",
    "        # (B, 1, 8, 16) -> (B, 16, 2, 4)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(8, 16, 3, 1, 1),\n",
    "            nn.ReLU(),\n",
    "            nn.AvgPool2d(2),\n",
    "            nn.Conv2d(16, 16, 3, 1, 1),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(2 * 4 * 16, 128),\n",
    "            nn.ReLU(),\n",
    "            # nn.Linear(128, 128),\n",
    "            # nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, signal_in):\n",
    "        return self.mlp(self.cnn(signal_in))\n",
    "\n",
    "class Detector_MLP(nn.Module):\n",
    "    def __init__(self, output_size: int = 4):\n",
    "        super(Detector_MLP, self).__init__()\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, signal_in):\n",
    "        return self.mlp(signal_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b6138b13-3c84-45d0-be85-2c391e907450",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize_regularize(model, device, alpha=0.01, interval=0.05):\n",
    "    central_params = torch.arange(0.0, 1+interval, interval).to(device)\n",
    "    reg_params = {}\n",
    "    for c in central_params:\n",
    "        if c == 0.0:\n",
    "            reg_params[c] = [p.abs() < interval / 2 for n, p in model.named_parameters()]\n",
    "        if c == 1.0:\n",
    "            reg_params[c] = [p.abs() > 1 - interval / 2 for n, p in model.named_parameters()]\n",
    "        else:\n",
    "            reg_params[c] = [(c - interval / 2 <= p.abs()) & (p.abs() < c + interval / 2) for n, p in model.named_parameters()]\n",
    "\n",
    "    reg_loss = 0.0\n",
    "    for c, params in reg_params.items():\n",
    "        reg_loss += alpha * torch.sum(torch.stack([torch.sum((p[params[i]].abs() - c).abs()) for i, (n, p) in enumerate(model.named_parameters())]))\n",
    "\n",
    "    return reg_loss\n",
    "\n",
    "# train one epoch\n",
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler):\n",
    "    model.train()\n",
    "    metric_logger = MetricLogger(delimiter=\"    \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    header = 'Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    for symbols, signals in metric_logger.log_every(data_loader, print_freq, header):\n",
    "        symbols, signals = symbols.to(device), signals.to(device).to(torch.float)\n",
    "\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            # equalized_output = model(tx_pilot, rx_pilot, rx_input)\n",
    "            output = model(signals)\n",
    "            loss = F.cross_entropy(output, symbols)\n",
    "\n",
    "            if epoch >= 10:\n",
    "                loss += quantize_regularize(model, device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "        # calculate binary accuracy\n",
    "        output_bits = torch.argmax(output, dim=-1)\n",
    "        accuracy = (output_bits == symbols).float().mean()\n",
    "        \n",
    "        metric_logger.update(loss=loss, lr=optimizer.param_groups[0][\"lr\"])\n",
    "        metric_logger.update(accuracy=accuracy)\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "def evaluate(model, data_loader, device, epoch, print_freq):\n",
    "    model.eval()\n",
    "    metric_logger = MetricLogger(delimiter=\"    \")\n",
    "    header = 'Test: Epoch: [{}]'.format(epoch)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with torch.amp.autocast('cuda'):\n",
    "            for symbols, signals in metric_logger.log_every(data_loader, print_freq, header):\n",
    "                symbols, signals = symbols.to(device), signals.to(device).to(torch.float)\n",
    "\n",
    "                output = model(signals)\n",
    "                loss = F.cross_entropy(output, symbols)\n",
    "\n",
    "                # calculate binary accuracy\n",
    "                output_bits = torch.argmax(output, dim=-1)\n",
    "                accuracy = (output_bits == symbols).float().mean()\n",
    "                \n",
    "                metric_logger.update(loss=loss)\n",
    "                metric_logger.update(accuracy=accuracy)\n",
    "\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8ba6ed9-1417-4972-8193-ba9a6f893ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(snr, mode, cnn, noise):\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "    # set seed\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    batch_size = 512\n",
    "    num_epoch = 100\n",
    "    lr = 0.001\n",
    "    lrf = 0.1\n",
    "\n",
    "    train_data_path = \"./Symbol_Detection/channel_Rayleigh/img/\" + str(snr) + \"/\" + mode + \"/filtered_data.mat\"\n",
    "    val_data_path = \"./Symbol_Detection/channel_Rayleigh/img/\" + str(snr) + \"/\" + mode + \"/filtered_data.mat\"\n",
    "\n",
    "    train_dataset = channel_dataset(train_data_path, use_noise=noise, is_train=True)\n",
    "    val_dataset = channel_dataset(val_data_path, use_noise=noise, is_train=False)\n",
    "\n",
    "    train_sampler = data.RandomSampler(train_dataset)\n",
    "    val_sampler = data.SequentialSampler(val_dataset)\n",
    "\n",
    "    train_loader = data.DataLoader(train_dataset, batch_size=batch_size, sampler=train_sampler, drop_last=True)\n",
    "    val_loader = data.DataLoader(val_dataset, batch_size=batch_size, sampler=val_sampler, drop_last=True)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    out_size = 4 if mode != 'qam16' else 16\n",
    "    \n",
    "    model = Detector_CNN(output_size=out_size).to(device) if cnn else Detector_MLP(output_size=out_size).to(device)\n",
    "\n",
    "    params_to_optimize = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params_to_optimize, lr=lr)\n",
    "    scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "    lf = lambda x: ((1 + math.cos(x * math.pi / num_epoch)) / 2) * (1 - lrf) + lrf\n",
    "    scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lf)\n",
    "    scheduler.last_epoch = 0\n",
    "\n",
    "    best_loss, best_epoch = math.inf, 0\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epoch):\n",
    "        train_dict = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=200, scaler=scaler)\n",
    "        val_dict = evaluate(model, val_loader, device, epoch, print_freq=200)\n",
    "        scheduler.step()\n",
    "\n",
    "        if epoch >= 20 and val_dict['loss'] < best_loss:\n",
    "            best_loss = val_dict['loss']\n",
    "            best_epoch = epoch\n",
    "            save_file = {\n",
    "                \"model\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "                \"epoch\": epoch\n",
    "            }\n",
    "            torch.save(save_file, \"./Symbol_Detection/channel_Rayleigh/img/\" + str(snr) + \"/\" + mode + \"/best_detection_model.pth\")\n",
    "            print(\"Save model from: \", epoch)\n",
    "\n",
    "    print(\"Total time: {}s\".format(int(time.time() - start_time)))\n",
    "    print(\"Best loss: {}\".format(best_loss), \";  Best Epoch: {}\".format(best_epoch))\n",
    "\n",
    "    # min, max of the model parameters\n",
    "    min_params, max_params = [], []\n",
    "    for p in model.parameters():\n",
    "        min_params.append(p.min().item())\n",
    "        max_params.append(p.max().item())\n",
    "    min_params, max_params = np.min(min_params), np.max(max_params)\n",
    "    print(f\"Min of model parameters: {min_params}, Max of model parameters: {max_params}\")\n",
    "    # plot histogram of model parameters\n",
    "    params = [p.detach().cpu().numpy().flatten() for p in model.parameters()]\n",
    "    plt.hist(np.hstack(params), bins=100)\n",
    "    plt.show()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9bbbc864-274f-428a-8fa4-31ca0339d0f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = True\n",
    "noise = True\n",
    "\n",
    "snr = 5\n",
    "mode = 'qam4'  # pam4 qam4 qam16 psk4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0673b4f2-bd31-4b7a-899c-5457d1372331",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]    [  0/156]    eta: 0:00:06    lr: 0.001000    loss: 1.3882 (1.3882)    accuracy: 0.2285 (0.2285)    time: 0.0426    data: 0.0380    max mem: 22\n",
      "Epoch: [0] Total time: 0:00:06\n",
      "Averaged stats: lr: 0.001000    loss: 0.0001 (0.5428)    accuracy: 1.0000 (0.7490)\n",
      "Test: Epoch: [0]    [  0/192]    eta: 0:00:07    loss: 0.0185 (0.0185)    accuracy: 0.9980 (0.9980)    time: 0.0369    data: 0.0361    max mem: 22\n",
      "Test: Epoch: [0] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0001 (0.0002)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [1]    [  0/156]    eta: 0:00:06    lr: 0.001000    loss: 0.0001 (0.0001)    accuracy: 1.0000 (1.0000)    time: 0.0394    data: 0.0363    max mem: 22\n",
      "Epoch: [1] Total time: 0:00:05\n",
      "Averaged stats: lr: 0.001000    loss: 0.0000 (0.0002)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [1]    [  0/192]    eta: 0:00:07    loss: 0.0207 (0.0207)    accuracy: 0.9980 (0.9980)    time: 0.0372    data: 0.0365    max mem: 22\n",
      "Test: Epoch: [1] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [2]    [  0/156]    eta: 0:00:06    lr: 0.000999    loss: 0.0000 (0.0000)    accuracy: 1.0000 (1.0000)    time: 0.0401    data: 0.0368    max mem: 22\n",
      "Epoch: [2] Total time: 0:00:05\n",
      "Averaged stats: lr: 0.000999    loss: 0.0000 (0.0002)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [2]    [  0/192]    eta: 0:00:06    loss: 0.0210 (0.0210)    accuracy: 0.9980 (0.9980)    time: 0.0364    data: 0.0357    max mem: 22\n",
      "Test: Epoch: [2] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [3]    [  0/156]    eta: 0:00:06    lr: 0.000998    loss: 0.0000 (0.0000)    accuracy: 1.0000 (1.0000)    time: 0.0422    data: 0.0394    max mem: 22\n",
      "Epoch: [3] Total time: 0:00:05\n",
      "Averaged stats: lr: 0.000998    loss: 0.0000 (0.0002)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [3]    [  0/192]    eta: 0:00:07    loss: 0.0211 (0.0211)    accuracy: 0.9980 (0.9980)    time: 0.0366    data: 0.0359    max mem: 22\n",
      "Test: Epoch: [3] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [4]    [  0/156]    eta: 0:00:06    lr: 0.000996    loss: 0.0000 (0.0000)    accuracy: 1.0000 (1.0000)    time: 0.0401    data: 0.0369    max mem: 22\n",
      "Epoch: [4] Total time: 0:00:05\n",
      "Averaged stats: lr: 0.000996    loss: 0.0000 (0.0002)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [4]    [  0/192]    eta: 0:00:07    loss: 0.0195 (0.0195)    accuracy: 0.9980 (0.9980)    time: 0.0368    data: 0.0361    max mem: 22\n",
      "Test: Epoch: [4] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [5]    [  0/156]    eta: 0:00:06    lr: 0.000994    loss: 0.0000 (0.0000)    accuracy: 1.0000 (1.0000)    time: 0.0399    data: 0.0372    max mem: 22\n",
      "Epoch: [5] Total time: 0:00:05\n",
      "Averaged stats: lr: 0.000994    loss: 0.0000 (0.0002)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [5]    [  0/192]    eta: 0:00:06    loss: 0.0206 (0.0206)    accuracy: 0.9980 (0.9980)    time: 0.0364    data: 0.0357    max mem: 22\n",
      "Test: Epoch: [5] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [6]    [  0/156]    eta: 0:00:06    lr: 0.000992    loss: 0.0000 (0.0000)    accuracy: 1.0000 (1.0000)    time: 0.0396    data: 0.0369    max mem: 22\n",
      "Epoch: [6] Total time: 0:00:05\n",
      "Averaged stats: lr: 0.000992    loss: 0.0000 (0.0002)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [6]    [  0/192]    eta: 0:00:07    loss: 0.0190 (0.0190)    accuracy: 0.9980 (0.9980)    time: 0.0366    data: 0.0359    max mem: 22\n",
      "Test: Epoch: [6] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [7]    [  0/156]    eta: 0:00:06    lr: 0.000989    loss: 0.0000 (0.0000)    accuracy: 1.0000 (1.0000)    time: 0.0402    data: 0.0375    max mem: 22\n",
      "Epoch: [7] Total time: 0:00:05\n",
      "Averaged stats: lr: 0.000989    loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [7]    [  0/192]    eta: 0:00:07    loss: 0.0207 (0.0207)    accuracy: 0.9980 (0.9980)    time: 0.0374    data: 0.0367    max mem: 22\n",
      "Test: Epoch: [7] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [8]    [  0/156]    eta: 0:00:06    lr: 0.000986    loss: 0.0000 (0.0000)    accuracy: 1.0000 (1.0000)    time: 0.0398    data: 0.0371    max mem: 22\n",
      "Epoch: [8] Total time: 0:00:05\n",
      "Averaged stats: lr: 0.000986    loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [8]    [  0/192]    eta: 0:00:07    loss: 0.0207 (0.0207)    accuracy: 0.9980 (0.9980)    time: 0.0370    data: 0.0363    max mem: 22\n",
      "Test: Epoch: [8] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [9]    [  0/156]    eta: 0:00:06    lr: 0.000982    loss: 0.0000 (0.0000)    accuracy: 1.0000 (1.0000)    time: 0.0400    data: 0.0374    max mem: 22\n",
      "Epoch: [9] Total time: 0:00:05\n",
      "Averaged stats: lr: 0.000982    loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [9]    [  0/192]    eta: 0:00:06    loss: 0.0202 (0.0202)    accuracy: 0.9980 (0.9980)    time: 0.0364    data: 0.0357    max mem: 22\n",
      "Test: Epoch: [9] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [10]    [  0/156]    eta: 0:00:16    lr: 0.000978    loss: 3.6780 (3.6780)    accuracy: 1.0000 (1.0000)    time: 0.1057    data: 0.0368    max mem: 22\n",
      "Epoch: [10] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000978    loss: 0.0770 (0.2776)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [10]    [  0/192]    eta: 0:00:07    loss: 0.0218 (0.0218)    accuracy: 0.9980 (0.9980)    time: 0.0412    data: 0.0405    max mem: 22\n",
      "Test: Epoch: [10] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [11]    [  0/156]    eta: 0:00:17    lr: 0.000973    loss: 0.0760 (0.0760)    accuracy: 1.0000 (1.0000)    time: 0.1100    data: 0.0373    max mem: 22\n",
      "Epoch: [11] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000973    loss: 0.0540 (0.0630)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [11]    [  0/192]    eta: 0:00:07    loss: 0.0216 (0.0216)    accuracy: 0.9980 (0.9980)    time: 0.0368    data: 0.0361    max mem: 22\n",
      "Test: Epoch: [11] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [12]    [  0/156]    eta: 0:00:16    lr: 0.000968    loss: 0.0528 (0.0528)    accuracy: 1.0000 (1.0000)    time: 0.1049    data: 0.0368    max mem: 22\n",
      "Epoch: [12] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000968    loss: 0.0452 (0.0488)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [12]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0366    data: 0.0359    max mem: 22\n",
      "Test: Epoch: [12] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [13]    [  0/156]    eta: 0:00:16    lr: 0.000963    loss: 0.0450 (0.0450)    accuracy: 1.0000 (1.0000)    time: 0.1058    data: 0.0375    max mem: 22\n",
      "Epoch: [13] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000963    loss: 0.0410 (0.0423)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [13]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0382    data: 0.0374    max mem: 22\n",
      "Test: Epoch: [13] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [14]    [  0/156]    eta: 0:00:16    lr: 0.000957    loss: 0.0398 (0.0398)    accuracy: 1.0000 (1.0000)    time: 0.1052    data: 0.0371    max mem: 22\n",
      "Epoch: [14] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000957    loss: 0.0376 (0.0385)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [14]    [  0/192]    eta: 0:00:07    loss: 0.0218 (0.0218)    accuracy: 0.9980 (0.9980)    time: 0.0371    data: 0.0363    max mem: 22\n",
      "Test: Epoch: [14] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [15]    [  0/156]    eta: 0:00:16    lr: 0.000951    loss: 0.0368 (0.0368)    accuracy: 1.0000 (1.0000)    time: 0.1053    data: 0.0368    max mem: 22\n",
      "Epoch: [15] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000951    loss: 0.0356 (0.0359)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [15]    [  0/192]    eta: 0:00:07    loss: 0.0218 (0.0218)    accuracy: 0.9980 (0.9980)    time: 0.0371    data: 0.0363    max mem: 22\n",
      "Test: Epoch: [15] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [16]    [  0/156]    eta: 0:00:16    lr: 0.000944    loss: 0.0356 (0.0356)    accuracy: 1.0000 (1.0000)    time: 0.1059    data: 0.0375    max mem: 22\n",
      "Epoch: [16] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000944    loss: 0.0338 (0.0343)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [16]    [  0/192]    eta: 0:00:07    loss: 0.0216 (0.0216)    accuracy: 0.9980 (0.9980)    time: 0.0377    data: 0.0368    max mem: 22\n",
      "Test: Epoch: [16] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [17]    [  0/156]    eta: 0:00:17    lr: 0.000937    loss: 0.0333 (0.0333)    accuracy: 1.0000 (1.0000)    time: 0.1112    data: 0.0371    max mem: 22\n",
      "Epoch: [17] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000937    loss: 0.0324 (0.0328)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [17]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0367    data: 0.0359    max mem: 22\n",
      "Test: Epoch: [17] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [18]    [  0/156]    eta: 0:00:16    lr: 0.000930    loss: 0.0321 (0.0321)    accuracy: 1.0000 (1.0000)    time: 0.1058    data: 0.0371    max mem: 22\n",
      "Epoch: [18] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000930    loss: 0.0315 (0.0316)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [18]    [  0/192]    eta: 0:00:07    loss: 0.0216 (0.0216)    accuracy: 0.9980 (0.9980)    time: 0.0369    data: 0.0361    max mem: 22\n",
      "Test: Epoch: [18] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [19]    [  0/156]    eta: 0:00:16    lr: 0.000922    loss: 0.0313 (0.0313)    accuracy: 1.0000 (1.0000)    time: 0.1052    data: 0.0370    max mem: 22\n",
      "Epoch: [19] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000922    loss: 0.0308 (0.0307)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [19]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0365    data: 0.0357    max mem: 22\n",
      "Test: Epoch: [19] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [20]    [  0/156]    eta: 0:00:17    lr: 0.000914    loss: 0.0303 (0.0303)    accuracy: 1.0000 (1.0000)    time: 0.1106    data: 0.0375    max mem: 22\n",
      "Epoch: [20] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000914    loss: 0.0305 (0.0298)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [20]    [  0/192]    eta: 0:00:07    loss: 0.0218 (0.0218)    accuracy: 0.9980 (0.9980)    time: 0.0400    data: 0.0392    max mem: 22\n",
      "Test: Epoch: [20] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Save model from:  20\n",
      "Epoch: [21]    [  0/156]    eta: 0:00:16    lr: 0.000906    loss: 0.0301 (0.0301)    accuracy: 1.0000 (1.0000)    time: 0.1059    data: 0.0370    max mem: 22\n",
      "Epoch: [21] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000906    loss: 0.0291 (0.0292)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [21]    [  0/192]    eta: 0:00:06    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0363    data: 0.0355    max mem: 22\n",
      "Test: Epoch: [21] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Save model from:  21\n",
      "Epoch: [22]    [  0/156]    eta: 0:00:17    lr: 0.000897    loss: 0.0286 (0.0286)    accuracy: 1.0000 (1.0000)    time: 0.1124    data: 0.0404    max mem: 22\n",
      "Epoch: [22] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000897    loss: 0.0287 (0.0284)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [22]    [  0/192]    eta: 0:00:06    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0364    data: 0.0356    max mem: 22\n",
      "Test: Epoch: [22] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Save model from:  22\n",
      "Epoch: [23]    [  0/156]    eta: 0:00:18    lr: 0.000888    loss: 0.0282 (0.0282)    accuracy: 1.0000 (1.0000)    time: 0.1178    data: 0.0381    max mem: 22\n",
      "Epoch: [23] Total time: 0:00:17\n",
      "Averaged stats: lr: 0.000888    loss: 0.0281 (0.0279)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [23]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0375    data: 0.0366    max mem: 22\n",
      "Test: Epoch: [23] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Save model from:  23\n",
      "Epoch: [24]    [  0/156]    eta: 0:00:17    lr: 0.000878    loss: 0.0277 (0.0277)    accuracy: 1.0000 (1.0000)    time: 0.1131    data: 0.0367    max mem: 22\n",
      "Epoch: [24] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000878    loss: 0.0275 (0.0273)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [24]    [  0/192]    eta: 0:00:06    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0364    data: 0.0357    max mem: 22\n",
      "Test: Epoch: [24] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Save model from:  24\n",
      "Epoch: [25]    [  0/156]    eta: 0:00:17    lr: 0.000868    loss: 0.0272 (0.0272)    accuracy: 1.0000 (1.0000)    time: 0.1095    data: 0.0370    max mem: 22\n",
      "Epoch: [25] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000868    loss: 0.0269 (0.0267)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [25]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0381    data: 0.0374    max mem: 22\n",
      "Test: Epoch: [25] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [26]    [  0/156]    eta: 0:00:16    lr: 0.000858    loss: 0.0264 (0.0264)    accuracy: 1.0000 (1.0000)    time: 0.1052    data: 0.0367    max mem: 22\n",
      "Epoch: [26] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000858    loss: 0.0266 (0.0263)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [26]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0367    data: 0.0360    max mem: 22\n",
      "Test: Epoch: [26] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [27]    [  0/156]    eta: 0:00:16    lr: 0.000848    loss: 0.0260 (0.0260)    accuracy: 1.0000 (1.0000)    time: 0.1066    data: 0.0371    max mem: 22\n",
      "Epoch: [27] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000848    loss: 0.0262 (0.0258)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [27]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0367    data: 0.0359    max mem: 22\n",
      "Test: Epoch: [27] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [28]    [  0/156]    eta: 0:00:17    lr: 0.000837    loss: 0.0257 (0.0257)    accuracy: 1.0000 (1.0000)    time: 0.1096    data: 0.0371    max mem: 22\n",
      "Epoch: [28] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000837    loss: 0.0259 (0.0253)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [28]    [  0/192]    eta: 0:00:07    loss: 0.0216 (0.0216)    accuracy: 0.9980 (0.9980)    time: 0.0389    data: 0.0381    max mem: 22\n",
      "Test: Epoch: [28] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Save model from:  28\n",
      "Epoch: [29]    [  0/156]    eta: 0:00:17    lr: 0.000826    loss: 0.0253 (0.0253)    accuracy: 1.0000 (1.0000)    time: 0.1111    data: 0.0384    max mem: 22\n",
      "Epoch: [29] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000826    loss: 0.0252 (0.0250)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [29]    [  0/192]    eta: 0:00:07    loss: 0.0216 (0.0216)    accuracy: 0.9980 (0.9980)    time: 0.0366    data: 0.0358    max mem: 22\n",
      "Test: Epoch: [29] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Save model from:  29\n",
      "Epoch: [30]    [  0/156]    eta: 0:00:16    lr: 0.000815    loss: 0.0247 (0.0247)    accuracy: 1.0000 (1.0000)    time: 0.1054    data: 0.0365    max mem: 22\n",
      "Epoch: [30] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000815    loss: 0.0248 (0.0245)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [30]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0373    data: 0.0365    max mem: 22\n",
      "Test: Epoch: [30] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [31]    [  0/156]    eta: 0:00:16    lr: 0.000803    loss: 0.0242 (0.0242)    accuracy: 1.0000 (1.0000)    time: 0.1064    data: 0.0370    max mem: 22\n",
      "Epoch: [31] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000803    loss: 0.0247 (0.0241)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [31]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0371    data: 0.0362    max mem: 22\n",
      "Test: Epoch: [31] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [32]    [  0/156]    eta: 0:00:17    lr: 0.000791    loss: 0.0240 (0.0240)    accuracy: 1.0000 (1.0000)    time: 0.1115    data: 0.0373    max mem: 22\n",
      "Epoch: [32] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000791    loss: 0.0238 (0.0237)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [32]    [  0/192]    eta: 0:00:06    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0364    data: 0.0356    max mem: 22\n",
      "Test: Epoch: [32] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [33]    [  0/156]    eta: 0:00:17    lr: 0.000779    loss: 0.0233 (0.0233)    accuracy: 1.0000 (1.0000)    time: 0.1096    data: 0.0365    max mem: 22\n",
      "Epoch: [33] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000779    loss: 0.0234 (0.0233)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [33]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0397    data: 0.0389    max mem: 22\n",
      "Test: Epoch: [33] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [34]    [  0/156]    eta: 0:00:17    lr: 0.000767    loss: 0.0228 (0.0228)    accuracy: 1.0000 (1.0000)    time: 0.1142    data: 0.0378    max mem: 22\n",
      "Epoch: [34] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000767    loss: 0.0234 (0.0229)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [34]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0386    data: 0.0378    max mem: 22\n",
      "Test: Epoch: [34] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [35]    [  0/156]    eta: 0:00:17    lr: 0.000754    loss: 0.0228 (0.0228)    accuracy: 1.0000 (1.0000)    time: 0.1125    data: 0.0376    max mem: 22\n",
      "Epoch: [35] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000754    loss: 0.0227 (0.0225)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [35]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0370    data: 0.0362    max mem: 22\n",
      "Test: Epoch: [35] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [36]    [  0/156]    eta: 0:00:16    lr: 0.000742    loss: 0.0221 (0.0221)    accuracy: 1.0000 (1.0000)    time: 0.1074    data: 0.0374    max mem: 22\n",
      "Epoch: [36] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000742    loss: 0.0220 (0.0221)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [36]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0369    data: 0.0362    max mem: 22\n",
      "Test: Epoch: [36] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [37]    [  0/156]    eta: 0:00:17    lr: 0.000729    loss: 0.0217 (0.0217)    accuracy: 1.0000 (1.0000)    time: 0.1103    data: 0.0375    max mem: 22\n",
      "Epoch: [37] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000729    loss: 0.0220 (0.0217)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [37]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0371    data: 0.0363    max mem: 22\n",
      "Test: Epoch: [37] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [38]    [  0/156]    eta: 0:00:17    lr: 0.000716    loss: 0.0214 (0.0214)    accuracy: 1.0000 (1.0000)    time: 0.1108    data: 0.0369    max mem: 22\n",
      "Epoch: [38] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000716    loss: 0.0216 (0.0213)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [38]    [  0/192]    eta: 0:00:07    loss: 0.0216 (0.0216)    accuracy: 0.9980 (0.9980)    time: 0.0369    data: 0.0362    max mem: 22\n",
      "Test: Epoch: [38] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [39]    [  0/156]    eta: 0:00:17    lr: 0.000702    loss: 0.0212 (0.0212)    accuracy: 1.0000 (1.0000)    time: 0.1132    data: 0.0394    max mem: 22\n",
      "Epoch: [39] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000702    loss: 0.0209 (0.0208)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [39]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0373    data: 0.0365    max mem: 22\n",
      "Test: Epoch: [39] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [40]    [  0/156]    eta: 0:00:17    lr: 0.000689    loss: 0.0205 (0.0205)    accuracy: 1.0000 (1.0000)    time: 0.1130    data: 0.0379    max mem: 22\n",
      "Epoch: [40] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000689    loss: 0.0205 (0.0204)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [40]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0367    data: 0.0360    max mem: 22\n",
      "Test: Epoch: [40] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [41]    [  0/156]    eta: 0:00:17    lr: 0.000676    loss: 0.0201 (0.0201)    accuracy: 1.0000 (1.0000)    time: 0.1106    data: 0.0374    max mem: 22\n",
      "Epoch: [41] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000676    loss: 0.0203 (0.0200)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [41]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0372    data: 0.0364    max mem: 22\n",
      "Test: Epoch: [41] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [42]    [  0/156]    eta: 0:00:17    lr: 0.000662    loss: 0.0199 (0.0199)    accuracy: 1.0000 (1.0000)    time: 0.1095    data: 0.0375    max mem: 22\n",
      "Epoch: [42] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000662    loss: 0.0200 (0.0196)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [42]    [  0/192]    eta: 0:00:07    loss: 0.0216 (0.0216)    accuracy: 0.9980 (0.9980)    time: 0.0376    data: 0.0368    max mem: 22\n",
      "Test: Epoch: [42] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Save model from:  42\n",
      "Epoch: [43]    [  0/156]    eta: 0:00:17    lr: 0.000648    loss: 0.0195 (0.0195)    accuracy: 1.0000 (1.0000)    time: 0.1124    data: 0.0396    max mem: 22\n",
      "Epoch: [43] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000648    loss: 0.0193 (0.0192)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [43]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0378    data: 0.0370    max mem: 22\n",
      "Test: Epoch: [43] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [44]    [  0/156]    eta: 0:00:16    lr: 0.000634    loss: 0.0189 (0.0189)    accuracy: 1.0000 (1.0000)    time: 0.1067    data: 0.0374    max mem: 22\n",
      "Epoch: [44] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000634    loss: 0.0191 (0.0188)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [44]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0373    data: 0.0365    max mem: 22\n",
      "Test: Epoch: [44] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [45]    [  0/156]    eta: 0:00:16    lr: 0.000620    loss: 0.0188 (0.0188)    accuracy: 1.0000 (1.0000)    time: 0.1072    data: 0.0377    max mem: 22\n",
      "Epoch: [45] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000620    loss: 0.0184 (0.0184)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [45]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0383    data: 0.0375    max mem: 22\n",
      "Test: Epoch: [45] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [46]    [  0/156]    eta: 0:00:19    lr: 0.000606    loss: 0.0182 (0.0182)    accuracy: 1.0000 (1.0000)    time: 0.1225    data: 0.0502    max mem: 22\n",
      "Epoch: [46] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000606    loss: 0.0180 (0.0180)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [46]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0371    data: 0.0363    max mem: 22\n",
      "Test: Epoch: [46] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [47]    [  0/156]    eta: 0:00:16    lr: 0.000592    loss: 0.0178 (0.0178)    accuracy: 1.0000 (1.0000)    time: 0.1071    data: 0.0372    max mem: 22\n",
      "Epoch: [47] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000592    loss: 0.0178 (0.0176)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [47]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0365    data: 0.0358    max mem: 22\n",
      "Test: Epoch: [47] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [48]    [  0/156]    eta: 0:00:18    lr: 0.000578    loss: 0.0177 (0.0177)    accuracy: 1.0000 (1.0000)    time: 0.1167    data: 0.0370    max mem: 22\n",
      "Epoch: [48] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000578    loss: 0.0172 (0.0172)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [48]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0365    data: 0.0358    max mem: 22\n",
      "Test: Epoch: [48] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [49]    [  0/156]    eta: 0:00:17    lr: 0.000564    loss: 0.0169 (0.0169)    accuracy: 1.0000 (1.0000)    time: 0.1111    data: 0.0375    max mem: 22\n",
      "Epoch: [49] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000564    loss: 0.0169 (0.0167)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [49]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0381    data: 0.0373    max mem: 22\n",
      "Test: Epoch: [49] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [50]    [  0/156]    eta: 0:00:16    lr: 0.000550    loss: 0.0167 (0.0167)    accuracy: 1.0000 (1.0000)    time: 0.1053    data: 0.0367    max mem: 22\n",
      "Epoch: [50] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000550    loss: 0.0165 (0.0163)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [50]    [  0/192]    eta: 0:00:07    loss: 0.0216 (0.0216)    accuracy: 0.9980 (0.9980)    time: 0.0369    data: 0.0361    max mem: 22\n",
      "Test: Epoch: [50] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Save model from:  50\n",
      "Epoch: [51]    [  0/156]    eta: 0:00:17    lr: 0.000536    loss: 0.0164 (0.0164)    accuracy: 1.0000 (1.0000)    time: 0.1096    data: 0.0377    max mem: 22\n",
      "Epoch: [51] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000536    loss: 0.0162 (0.0158)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [51]    [  0/192]    eta: 0:00:06    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0363    data: 0.0355    max mem: 22\n",
      "Test: Epoch: [51] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [52]    [  0/156]    eta: 0:00:17    lr: 0.000522    loss: 0.0160 (0.0160)    accuracy: 1.0000 (1.0000)    time: 0.1092    data: 0.0367    max mem: 22\n",
      "Epoch: [52] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000522    loss: 0.0156 (0.0154)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [52]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0371    data: 0.0363    max mem: 22\n",
      "Test: Epoch: [52] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [53]    [  0/156]    eta: 0:00:16    lr: 0.000508    loss: 0.0155 (0.0155)    accuracy: 1.0000 (1.0000)    time: 0.1051    data: 0.0369    max mem: 22\n",
      "Epoch: [53] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000508    loss: 0.0150 (0.0151)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [53]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0380    data: 0.0372    max mem: 22\n",
      "Test: Epoch: [53] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [54]    [  0/156]    eta: 0:00:16    lr: 0.000494    loss: 0.0149 (0.0149)    accuracy: 1.0000 (1.0000)    time: 0.1062    data: 0.0372    max mem: 22\n",
      "Epoch: [54] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000494    loss: 0.0149 (0.0146)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [54]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0375    data: 0.0367    max mem: 22\n",
      "Test: Epoch: [54] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [55]    [  0/156]    eta: 0:00:17    lr: 0.000480    loss: 0.0149 (0.0149)    accuracy: 1.0000 (1.0000)    time: 0.1095    data: 0.0372    max mem: 22\n",
      "Epoch: [55] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000480    loss: 0.0141 (0.0143)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [55]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0379    data: 0.0370    max mem: 22\n",
      "Test: Epoch: [55] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [56]    [  0/156]    eta: 0:00:16    lr: 0.000466    loss: 0.0142 (0.0142)    accuracy: 1.0000 (1.0000)    time: 0.1063    data: 0.0374    max mem: 22\n",
      "Epoch: [56] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000466    loss: 0.0138 (0.0139)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [56]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0372    data: 0.0364    max mem: 22\n",
      "Test: Epoch: [56] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [57]    [  0/156]    eta: 0:00:16    lr: 0.000452    loss: 0.0138 (0.0138)    accuracy: 1.0000 (1.0000)    time: 0.1071    data: 0.0377    max mem: 22\n",
      "Epoch: [57] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000452    loss: 0.0133 (0.0135)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [57]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0386    data: 0.0378    max mem: 22\n",
      "Test: Epoch: [57] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [58]    [  0/156]    eta: 0:00:17    lr: 0.000438    loss: 0.0133 (0.0133)    accuracy: 1.0000 (1.0000)    time: 0.1103    data: 0.0369    max mem: 22\n",
      "Epoch: [58] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000438    loss: 0.0341 (0.0130)    accuracy: 0.9980 (1.0000)\n",
      "Test: Epoch: [58]    [  0/192]    eta: 0:00:07    loss: 0.0216 (0.0216)    accuracy: 0.9980 (0.9980)    time: 0.0367    data: 0.0359    max mem: 22\n",
      "Test: Epoch: [58] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Save model from:  58\n",
      "Epoch: [59]    [  0/156]    eta: 0:00:17    lr: 0.000424    loss: 0.0129 (0.0129)    accuracy: 1.0000 (1.0000)    time: 0.1138    data: 0.0392    max mem: 22\n",
      "Epoch: [59] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000424    loss: 0.0123 (0.0127)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [59]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0365    data: 0.0357    max mem: 22\n",
      "Test: Epoch: [59] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [60]    [  0/156]    eta: 0:00:16    lr: 0.000411    loss: 0.0124 (0.0124)    accuracy: 1.0000 (1.0000)    time: 0.1060    data: 0.0379    max mem: 22\n",
      "Epoch: [60] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000411    loss: 0.0121 (0.0122)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [60]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0371    data: 0.0363    max mem: 22\n",
      "Test: Epoch: [60] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [61]    [  0/156]    eta: 0:00:16    lr: 0.000398    loss: 0.0121 (0.0121)    accuracy: 1.0000 (1.0000)    time: 0.1067    data: 0.0379    max mem: 22\n",
      "Epoch: [61] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000398    loss: 0.0115 (0.0119)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [61]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0371    data: 0.0364    max mem: 22\n",
      "Test: Epoch: [61] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [62]    [  0/156]    eta: 0:00:17    lr: 0.000384    loss: 0.0117 (0.0117)    accuracy: 1.0000 (1.0000)    time: 0.1093    data: 0.0376    max mem: 22\n",
      "Epoch: [62] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000384    loss: 0.0111 (0.0115)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [62]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0368    data: 0.0361    max mem: 22\n",
      "Test: Epoch: [62] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [63]    [  0/156]    eta: 0:00:16    lr: 0.000371    loss: 0.0112 (0.0112)    accuracy: 1.0000 (1.0000)    time: 0.1054    data: 0.0371    max mem: 22\n",
      "Epoch: [63] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000371    loss: 0.0109 (0.0111)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [63]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0371    data: 0.0364    max mem: 22\n",
      "Test: Epoch: [63] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [64]    [  0/156]    eta: 0:00:16    lr: 0.000358    loss: 0.0109 (0.0109)    accuracy: 1.0000 (1.0000)    time: 0.1063    data: 0.0369    max mem: 22\n",
      "Epoch: [64] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000358    loss: 0.0107 (0.0107)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [64]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0365    data: 0.0357    max mem: 22\n",
      "Test: Epoch: [64] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [65]    [  0/156]    eta: 0:00:16    lr: 0.000346    loss: 0.0106 (0.0106)    accuracy: 1.0000 (1.0000)    time: 0.1070    data: 0.0374    max mem: 22\n",
      "Epoch: [65] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000346    loss: 0.0102 (0.0103)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [65]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0369    data: 0.0362    max mem: 22\n",
      "Test: Epoch: [65] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [66]    [  0/156]    eta: 0:00:17    lr: 0.000333    loss: 0.0101 (0.0101)    accuracy: 1.0000 (1.0000)    time: 0.1108    data: 0.0378    max mem: 22\n",
      "Epoch: [66] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000333    loss: 0.0100 (0.0100)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [66]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0368    data: 0.0360    max mem: 22\n",
      "Test: Epoch: [66] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [67]    [  0/156]    eta: 0:00:16    lr: 0.000321    loss: 0.0098 (0.0098)    accuracy: 1.0000 (1.0000)    time: 0.1066    data: 0.0376    max mem: 22\n",
      "Epoch: [67] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000321    loss: 0.0095 (0.0096)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [67]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0373    data: 0.0365    max mem: 22\n",
      "Test: Epoch: [67] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [68]    [  0/156]    eta: 0:00:16    lr: 0.000309    loss: 0.0094 (0.0094)    accuracy: 1.0000 (1.0000)    time: 0.1059    data: 0.0371    max mem: 22\n",
      "Epoch: [68] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000309    loss: 0.0091 (0.0093)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [68]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0370    data: 0.0362    max mem: 22\n",
      "Test: Epoch: [68] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [69]    [  0/156]    eta: 0:00:17    lr: 0.000297    loss: 0.0089 (0.0089)    accuracy: 1.0000 (1.0000)    time: 0.1114    data: 0.0377    max mem: 22\n",
      "Epoch: [69] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000297    loss: 0.0088 (0.0089)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [69]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0371    data: 0.0363    max mem: 22\n",
      "Test: Epoch: [69] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [70]    [  0/156]    eta: 0:00:16    lr: 0.000285    loss: 0.0086 (0.0086)    accuracy: 1.0000 (1.0000)    time: 0.1083    data: 0.0383    max mem: 22\n",
      "Epoch: [70] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000285    loss: 0.0085 (0.0086)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [70]    [  0/192]    eta: 0:00:07    loss: 0.0216 (0.0216)    accuracy: 0.9980 (0.9980)    time: 0.0370    data: 0.0362    max mem: 22\n",
      "Test: Epoch: [70] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [71]    [  0/156]    eta: 0:00:16    lr: 0.000274    loss: 0.0083 (0.0083)    accuracy: 1.0000 (1.0000)    time: 0.1071    data: 0.0377    max mem: 22\n",
      "Epoch: [71] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000274    loss: 0.0081 (0.0083)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [71]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0366    data: 0.0358    max mem: 22\n",
      "Test: Epoch: [71] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [72]    [  0/156]    eta: 0:00:16    lr: 0.000263    loss: 0.0079 (0.0079)    accuracy: 1.0000 (1.0000)    time: 0.1051    data: 0.0366    max mem: 22\n",
      "Epoch: [72] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000263    loss: 0.0077 (0.0080)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [72]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0374    data: 0.0366    max mem: 22\n",
      "Test: Epoch: [72] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [73]    [  0/156]    eta: 0:00:16    lr: 0.000252    loss: 0.0076 (0.0076)    accuracy: 1.0000 (1.0000)    time: 0.1062    data: 0.0376    max mem: 22\n",
      "Epoch: [73] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000252    loss: 0.0074 (0.0076)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [73]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0370    data: 0.0362    max mem: 22\n",
      "Test: Epoch: [73] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [74]    [  0/156]    eta: 0:00:16    lr: 0.000242    loss: 0.0073 (0.0073)    accuracy: 1.0000 (1.0000)    time: 0.1077    data: 0.0375    max mem: 22\n",
      "Epoch: [74] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000242    loss: 0.0071 (0.0073)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [74]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0378    data: 0.0370    max mem: 22\n",
      "Test: Epoch: [74] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [75]    [  0/156]    eta: 0:00:16    lr: 0.000232    loss: 0.0069 (0.0069)    accuracy: 1.0000 (1.0000)    time: 0.1049    data: 0.0367    max mem: 22\n",
      "Epoch: [75] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000232    loss: 0.0068 (0.0070)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [75]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0371    data: 0.0363    max mem: 22\n",
      "Test: Epoch: [75] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [76]    [  0/156]    eta: 0:00:16    lr: 0.000222    loss: 0.0067 (0.0067)    accuracy: 1.0000 (1.0000)    time: 0.1060    data: 0.0372    max mem: 22\n",
      "Epoch: [76] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000222    loss: 0.0065 (0.0067)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [76]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0396    data: 0.0388    max mem: 22\n",
      "Test: Epoch: [76] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [77]    [  0/156]    eta: 0:00:16    lr: 0.000212    loss: 0.0063 (0.0063)    accuracy: 1.0000 (1.0000)    time: 0.1079    data: 0.0378    max mem: 22\n",
      "Epoch: [77] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000212    loss: 0.0063 (0.0064)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [77]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0370    data: 0.0362    max mem: 22\n",
      "Test: Epoch: [77] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [78]    [  0/156]    eta: 0:00:17    lr: 0.000203    loss: 0.0062 (0.0062)    accuracy: 1.0000 (1.0000)    time: 0.1110    data: 0.0369    max mem: 22\n",
      "Epoch: [78] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000203    loss: 0.0059 (0.0061)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [78]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0367    data: 0.0360    max mem: 22\n",
      "Test: Epoch: [78] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [79]    [  0/156]    eta: 0:00:16    lr: 0.000194    loss: 0.0058 (0.0058)    accuracy: 1.0000 (1.0000)    time: 0.1062    data: 0.0371    max mem: 22\n",
      "Epoch: [79] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000194    loss: 0.0058 (0.0059)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [79]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0369    data: 0.0361    max mem: 22\n",
      "Test: Epoch: [79] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [80]    [  0/156]    eta: 0:00:16    lr: 0.000186    loss: 0.0056 (0.0056)    accuracy: 1.0000 (1.0000)    time: 0.1063    data: 0.0371    max mem: 22\n",
      "Epoch: [80] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000186    loss: 0.0054 (0.0056)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [80]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0368    data: 0.0361    max mem: 22\n",
      "Test: Epoch: [80] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [81]    [  0/156]    eta: 0:00:16    lr: 0.000178    loss: 0.0053 (0.0053)    accuracy: 1.0000 (1.0000)    time: 0.1062    data: 0.0373    max mem: 22\n",
      "Epoch: [81] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000178    loss: 0.0052 (0.0054)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [81]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0384    data: 0.0377    max mem: 22\n",
      "Test: Epoch: [81] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [82]    [  0/156]    eta: 0:00:17    lr: 0.000170    loss: 0.0051 (0.0051)    accuracy: 1.0000 (1.0000)    time: 0.1090    data: 0.0375    max mem: 22\n",
      "Epoch: [82] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000170    loss: 0.0049 (0.0051)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [82]    [  0/192]    eta: 0:00:07    loss: 0.0216 (0.0216)    accuracy: 0.9980 (0.9980)    time: 0.0372    data: 0.0365    max mem: 22\n",
      "Test: Epoch: [82] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [83]    [  0/156]    eta: 0:00:17    lr: 0.000163    loss: 0.0049 (0.0049)    accuracy: 1.0000 (1.0000)    time: 0.1122    data: 0.0379    max mem: 22\n",
      "Epoch: [83] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000163    loss: 0.0047 (0.0050)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [83]    [  0/192]    eta: 0:00:07    loss: 0.0216 (0.0216)    accuracy: 0.9980 (0.9980)    time: 0.0388    data: 0.0379    max mem: 22\n",
      "Test: Epoch: [83] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [84]    [  0/156]    eta: 0:00:17    lr: 0.000156    loss: 0.0047 (0.0047)    accuracy: 1.0000 (1.0000)    time: 0.1114    data: 0.0379    max mem: 22\n",
      "Epoch: [84] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000156    loss: 0.0045 (0.0047)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [84]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0369    data: 0.0362    max mem: 22\n",
      "Test: Epoch: [84] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [85]    [  0/156]    eta: 0:00:16    lr: 0.000149    loss: 0.0045 (0.0045)    accuracy: 1.0000 (1.0000)    time: 0.1069    data: 0.0374    max mem: 22\n",
      "Epoch: [85] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000149    loss: 0.0043 (0.0045)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [85]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0380    data: 0.0373    max mem: 22\n",
      "Test: Epoch: [85] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [86]    [  0/156]    eta: 0:00:16    lr: 0.000143    loss: 0.0043 (0.0043)    accuracy: 1.0000 (1.0000)    time: 0.1049    data: 0.0367    max mem: 22\n",
      "Epoch: [86] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000143    loss: 0.0042 (0.0044)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [86]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0371    data: 0.0364    max mem: 22\n",
      "Test: Epoch: [86] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [87]    [  0/156]    eta: 0:00:16    lr: 0.000137    loss: 0.0042 (0.0042)    accuracy: 1.0000 (1.0000)    time: 0.1059    data: 0.0373    max mem: 22\n",
      "Epoch: [87] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000137    loss: 0.0039 (0.0042)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [87]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0368    data: 0.0361    max mem: 22\n",
      "Test: Epoch: [87] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [88]    [  0/156]    eta: 0:00:16    lr: 0.000132    loss: 0.0039 (0.0039)    accuracy: 1.0000 (1.0000)    time: 0.1070    data: 0.0377    max mem: 22\n",
      "Epoch: [88] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000132    loss: 0.0038 (0.0040)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [88]    [  0/192]    eta: 0:00:06    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0363    data: 0.0355    max mem: 22\n",
      "Test: Epoch: [88] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [89]    [  0/156]    eta: 0:00:16    lr: 0.000127    loss: 0.0038 (0.0038)    accuracy: 1.0000 (1.0000)    time: 0.1059    data: 0.0365    max mem: 22\n",
      "Epoch: [89] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000127    loss: 0.0038 (0.0039)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [89]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0389    data: 0.0381    max mem: 22\n",
      "Test: Epoch: [89] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [90]    [  0/156]    eta: 0:00:18    lr: 0.000122    loss: 0.0038 (0.0038)    accuracy: 1.0000 (1.0000)    time: 0.1160    data: 0.0397    max mem: 22\n",
      "Epoch: [90] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000122    loss: 0.0037 (0.0038)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [90]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0366    data: 0.0358    max mem: 22\n",
      "Test: Epoch: [90] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [91]    [  0/156]    eta: 0:00:17    lr: 0.000118    loss: 0.0036 (0.0036)    accuracy: 1.0000 (1.0000)    time: 0.1102    data: 0.0368    max mem: 22\n",
      "Epoch: [91] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000118    loss: 0.0034 (0.0036)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [91]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0382    data: 0.0374    max mem: 22\n",
      "Test: Epoch: [91] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [92]    [  0/156]    eta: 0:00:17    lr: 0.000114    loss: 0.0034 (0.0034)    accuracy: 1.0000 (1.0000)    time: 0.1122    data: 0.0371    max mem: 22\n",
      "Epoch: [92] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000114    loss: 0.0034 (0.0035)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [92]    [  0/192]    eta: 0:00:06    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0364    data: 0.0356    max mem: 22\n",
      "Test: Epoch: [92] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [93]    [  0/156]    eta: 0:00:16    lr: 0.000111    loss: 0.0034 (0.0034)    accuracy: 1.0000 (1.0000)    time: 0.1068    data: 0.0379    max mem: 22\n",
      "Epoch: [93] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000111    loss: 0.0033 (0.0034)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [93]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0366    data: 0.0359    max mem: 22\n",
      "Test: Epoch: [93] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [94]    [  0/156]    eta: 0:00:16    lr: 0.000108    loss: 0.0033 (0.0033)    accuracy: 1.0000 (1.0000)    time: 0.1050    data: 0.0370    max mem: 22\n",
      "Epoch: [94] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000108    loss: 0.0032 (0.0033)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [94]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0373    data: 0.0365    max mem: 22\n",
      "Test: Epoch: [94] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [95]    [  0/156]    eta: 0:00:16    lr: 0.000106    loss: 0.0031 (0.0031)    accuracy: 1.0000 (1.0000)    time: 0.1077    data: 0.0374    max mem: 22\n",
      "Epoch: [95] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000106    loss: 0.0031 (0.0033)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [95]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0370    data: 0.0362    max mem: 22\n",
      "Test: Epoch: [95] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [96]    [  0/156]    eta: 0:00:16    lr: 0.000104    loss: 0.0031 (0.0031)    accuracy: 1.0000 (1.0000)    time: 0.1064    data: 0.0376    max mem: 22\n",
      "Epoch: [96] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000104    loss: 0.0030 (0.0032)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [96]    [  0/192]    eta: 0:00:06    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0364    data: 0.0356    max mem: 22\n",
      "Test: Epoch: [96] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [97]    [  0/156]    eta: 0:00:16    lr: 0.000102    loss: 0.0030 (0.0030)    accuracy: 1.0000 (1.0000)    time: 0.1086    data: 0.0376    max mem: 22\n",
      "Epoch: [97] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000102    loss: 0.0031 (0.0031)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [97]    [  0/192]    eta: 0:00:06    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0364    data: 0.0357    max mem: 22\n",
      "Test: Epoch: [97] Total time: 0:00:06\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [98]    [  0/156]    eta: 0:00:17    lr: 0.000101    loss: 0.0030 (0.0030)    accuracy: 1.0000 (1.0000)    time: 0.1121    data: 0.0382    max mem: 22\n",
      "Epoch: [98] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000101    loss: 0.0030 (0.0031)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [98]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0366    data: 0.0358    max mem: 22\n",
      "Test: Epoch: [98] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Epoch: [99]    [  0/156]    eta: 0:00:17    lr: 0.000100    loss: 0.0030 (0.0030)    accuracy: 1.0000 (1.0000)    time: 0.1139    data: 0.0375    max mem: 22\n",
      "Epoch: [99] Total time: 0:00:16\n",
      "Averaged stats: lr: 0.000100    loss: 0.0030 (0.0031)    accuracy: 1.0000 (1.0000)\n",
      "Test: Epoch: [99]    [  0/192]    eta: 0:00:07    loss: 0.0217 (0.0217)    accuracy: 0.9980 (0.9980)    time: 0.0370    data: 0.0362    max mem: 22\n",
      "Test: Epoch: [99] Total time: 0:00:07\n",
      "Averaged stats: loss: 0.0000 (0.0001)    accuracy: 1.0000 (1.0000)\n",
      "Total time: 2254s\n",
      "Best loss: 0.00011959102024169017 ;  Best Epoch: 58\n",
      "Min of model parameters: -0.5999993681907654, Max of model parameters: 0.699978232383728\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "main(snr, mode, cnn, noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b33865de-23bb-447d-9d6e-48081eb61fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "out_size = 4 if mode != 'qam16' else 16\n",
    "\n",
    "model = Detector_CNN(output_size=out_size).to(device) if cnn else Detector_MLP(output_size=out_size).to(device)\n",
    "model_path = \"./Symbol_Detection/channel_Rayleigh/img/\" + str(snr) + \"/\" + mode + \"/best_detection_model.pth\"\n",
    "ckpt = torch.load(model_path)['model']\n",
    "# for n, p in ckpt.items():\n",
    "#     print(n, p.shape)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()\n",
    "\n",
    "val_data_path = \"./Symbol_Detection/channel_Rayleigh/img/\" + str(snr) + \"/\" + mode + \"/filtered_data.mat\"\n",
    "val_dataset = channel_dataset(val_data_path, use_noise=noise, is_train=False)\n",
    "val_sampler = data.SequentialSampler(val_dataset)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=64, sampler=val_sampler, drop_last=False)\n",
    "\n",
    "dummy_input = torch.randn(1, 1, 8, 16).to(device)\n",
    "torch.onnx.export(model, dummy_input,\n",
    "                  \"./Symbol_Detection/channel_Rayleigh/img/\" + str(snr) + \"/\" + mode + \"/symbol_detection.onnx\",\n",
    "                  export_params=True, opset_version=16, dynamic_axes={'input': {0: 'batch_size'}, 'sensor_out': {0: 'batch_size'}},\n",
    "                  input_names=['input'], output_names=['sensor_out'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8934e8df-2494-4147-a241-29d7ed4cde56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9962128812199037 0.9960623996789727\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "out_size = 4 if mode != 'qam16' else 16\n",
    "\n",
    "model = Detector_CNN(output_size=out_size).to(device) if cnn else Detector_MLP(output_size=out_size).to(device)\n",
    "model_path = \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/best_detection_model.pth\"\n",
    "ckpt = torch.load(model_path)['model']\n",
    "# for n, p in ckpt.items():\n",
    "#     print(n, p.shape)\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()\n",
    "\n",
    "train_data_path = \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/shuffle/filtered_data_train.mat\"\n",
    "val_data_path = \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/shuffle/filtered_data_val.mat\"\n",
    "\n",
    "train_dataset = channel_dataset(train_data_path, use_noise=noise, is_train=True)\n",
    "val_dataset = channel_dataset(val_data_path, use_noise=noise, is_train=False)\n",
    "\n",
    "train_sampler = data.SequentialSampler(train_dataset)\n",
    "val_sampler = data.SequentialSampler(val_dataset)\n",
    "\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=64, sampler=train_sampler, drop_last=False)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=64, sampler=val_sampler, drop_last=False)\n",
    "\n",
    "simulate_outputs = []\n",
    "true_symbols = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast('cuda'):\n",
    "        for symbols, signals in train_loader:\n",
    "            symbols, signals = symbols.to(device), signals.to(device).to(torch.float)\n",
    "\n",
    "            output = model(signals)\n",
    "\n",
    "            true_symbols.append(symbols.detach().cpu().numpy())\n",
    "            simulate_outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.amp.autocast('cuda'):\n",
    "        for symbols, signals in val_loader:\n",
    "            symbols, signals = symbols.to(device), signals.to(device).to(torch.float)\n",
    "\n",
    "            output = model(signals)\n",
    "\n",
    "            true_symbols.append(symbols.detach().cpu().numpy())\n",
    "            simulate_outputs.append(output.detach().cpu().numpy())\n",
    "\n",
    "simulate_outputs = np.concatenate(simulate_outputs, 0)  # (B, 4)\n",
    "true_symbols = np.concatenate(true_symbols, 0)  # (B,)\n",
    "\n",
    "HW_data_path = \"./Symbol_Detection/channel_Rayleigh/\" + str(snr) + \"/\" + mode + \"/hw_out_\" + str(snr) +\"db_\" + mode + \".npy\"\n",
    "HW_data = np.load(HW_data_path)  # (B, 4)\n",
    "\n",
    "simulate_acc = (np.argmax(simulate_outputs, -1) == true_symbols).mean()\n",
    "HW_acc = (np.argmax(HW_data, -1) == true_symbols).mean()\n",
    "\n",
    "print(simulate_acc, HW_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4ed7542f-b4d2-4329-8db3-d3de0d4b9f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21264,)\n"
     ]
    }
   ],
   "source": [
    "HW_data_path = \"./Symbol_Detection/channel_Rayleigh/img/\" + str(snr) + \"/\" + mode + \"/symbol_img_hw_out_5db_qam4.npy\"\n",
    "HW_data = np.load(HW_data_path)  # (B, 4)\n",
    "HW_data = np.argmax(HW_data, 1)\n",
    "print(HW_data.shape)\n",
    "path = \"./Symbol_Detection/channel_Rayleigh/img/\" + str(snr) + \"/\" + mode + \"/symbol_img_hw_out_5db_qam4.mat\"\n",
    "data_dict = {\n",
    "    \"HW_data\": HW_data\n",
    "}\n",
    "sio.savemat(path, data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c75c3e9-19b0-44f2-ad54-1813f8495ea3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
